[
    {
        "id": "6811826e-c145-4dd8-b155-7ae6cf3e1148",
        "title": "",
        "chunk_text": "1 Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-2n/2 Enumeration Jesus Salas Independent Researcher jesus.salas@gmail|microsoft.com https://orcid.org/0009-0007-6411-2270 Note This is a preliminary, extended version posted on arXiv. A shorter version may appear in a conference. We welcome peer reviews, suggestions, or alternative analyses.",
        "metadata": {
            "author": "",
            "keywords": [
                "Sum",
                "Adaptive",
                "Enumeration",
                "Note",
                "gmail",
                "microsoft.com",
                "Subset",
                "Solver",
                "Jesus",
                "Salas"
            ]
        }
    },
    {
        "id": "dd7777ff-361f-467d-ac0f-5df84205f3ee",
        "title": "",
        "chunk_text": "Abstract The Subset Sum problem, which asks whether a given set of n integers contains a subset summing to a specified target t, is a fundamental NP-complete problem arising in cryptography, combinato- rial optimization, and beyond. The classical meet-in-the-middle (MIM) algorithm of Horowitz and Sahni achieves a worst-case time complexity of e O(2n/2), which remains the best-known deterministic bound.",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "problem",
                "combinato",
                "Abstract",
                "cryptography",
                "rial",
                "optimization",
                "MIM",
                "set"
            ]
        }
    },
    {
        "id": "3989b1f1-e2b3-4887-9a43-364db59c964e",
        "title": "",
        "chunk_text": "Yet many instances exhibit abundant collisions in partial sums, so the true difficulty is often governed by the number of unique subset sums (U) rather than the full 2n subsets. We present a structure-aware, adaptive solver that deterministically enumerates only the distinct sums, pruning duplicates on the fly and thus operating in time proportional to e O(U) whenever U ≪2n.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "full",
                "instances",
                "exhibit",
                "abundant",
                "collisions",
                "partial",
                "true",
                "difficulty",
                "governed"
            ]
        }
    },
    {
        "id": "b9a99021-6821-461e-9f5c-622f874a6988",
        "title": "",
        "chunk_text": "Its core is a unique-subset-sums enumerator combined with a double meet-in- the-middle strategy and lightweight dynamic programming, thereby avoiding the classical MIM’s expensive merging phase. Additionally, we introduce a combinatorial tree compression technique that ensures strictly sub-2n/2 enumeration even on unstructured instances, effectively reducing the exponent by a small but nontrivial constant factor.",
        "metadata": {
            "author": "",
            "keywords": [
                "MIM",
                "enumerator",
                "meet-in",
                "the-middle",
                "programming",
                "phase",
                "core",
                "combined",
                "double",
                "strategy"
            ]
        }
    },
    {
        "id": "5bbb6890-09cd-40b0-b190-7655212daa44",
        "title": "",
        "chunk_text": "Our solver supports anytime and online operation, producing intermediate solutions (partial expansions) early and adapting seamlessly to newly added elements. Both theoretical analysis and empirical evidence show that for structured inputs—such as those with small doubling constants, high additive energy, or significant additive redundancy—our method can significantly outperform classical approaches, often approaching near-dynamic programming efficiency.",
        "metadata": {
            "author": "",
            "keywords": [
                "operation",
                "producing",
                "solutions",
                "partial",
                "expansions",
                "early",
                "elements",
                "solver",
                "supports",
                "anytime"
            ]
        }
    },
    {
        "id": "2b33e34f-d7bf-4d62-84ea-9b392fbe80f9",
        "title": "",
        "chunk_text": "Even in the worst- case regime, it never exceeds the e O(2n/2) bound, and its compression-based pruning guarantees a genuine constant-factor speedup over naive MIM enumerations. We conclude by discussing how this instance-specific adaptivity refines the Subset Sum complexity landscape, and how our unified, output-sensitive framework could inspire further advances in adaptive exponential-time algorithms.",
        "metadata": {
            "author": "",
            "keywords": [
                "bound",
                "MIM",
                "worst",
                "case",
                "regime",
                "enumerations",
                "exceeds",
                "compression-based",
                "pruning",
                "guarantees"
            ]
        }
    },
    {
        "id": "076524d7-9e16-4aef-9609-d91baf1d02d1",
        "title": "",
        "chunk_text": "2012 ACM Subject Classification NP-complete problems, Design and analysis of algorithms Keywords and phrases Subset Sum, NP-Completeness, Adaptive Algorithms, Additive Combina- torics, Exponential Time Algorithms, Instance Hardness 1 Introduction The Subset Sum problem is a classical NP-complete problem that asks whether a given set S of n integers contains a subset whose elements sum to a specified target t.",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "algorithms",
                "ACM",
                "Design",
                "Adaptive",
                "Additive",
                "Combina",
                "Exponential",
                "Instance"
            ]
        }
    },
    {
        "id": "d458dab4-44e8-4b34-9921-285a957c26d3",
        "title": "",
        "chunk_text": "Traditionally, arXiv:2503.20162v1 [cs.DS] 26 Mar 2025 2 worst-case complexity is measured in terms of n and the total number of potential subsets, i.e., 2n. However, in many practical scenarios, the input exhibits significant additive struc- ture—captured by parameters such as the doubling constant or additive energy—which implies that many subsets yield the same sum.",
        "metadata": {
            "author": "",
            "keywords": [
                "Mar",
                "Traditionally",
                "arXiv",
                "cs.DS",
                "worst-case",
                "subsets",
                "complexity",
                "measured",
                "terms",
                "total"
            ]
        }
    },
    {
        "id": "d3934fe6-13cd-434e-b6ff-23ff62bae502",
        "title": "",
        "chunk_text": "In such cases, the true computational challenge is determined not by n alone but by the effective search space U, defined as the number of distinct subset sums in Σ(S). The seminal meet-in-the-middle algorithm of Horowitz and Sahni [1] achieves a worst-case running time of e O(2n/2) by dividing S into two lists and enumerating all subset sums for each.",
        "metadata": {
            "author": "",
            "keywords": [
                "cases",
                "defined",
                "subset",
                "sums",
                "true",
                "computational",
                "challenge",
                "determined",
                "effective",
                "search"
            ]
        }
    },
    {
        "id": "ef8ee334-48ab-421b-8d0b-321b91a841bb",
        "title": "",
        "chunk_text": "Although this bound remains tight in theory, empirical studies and recent research [2, 3] suggest that the practical difficulty of an instance is better characterized by the number of unique subset sums rather than the total number of subsets. For example, when the input set has a low doubling constant or high additive energy, many subset sums coincide, reducing U and, consequently, the work required to solve the instance.",
        "metadata": {
            "author": "",
            "keywords": [
                "number",
                "sums",
                "subset",
                "theory",
                "empirical",
                "research",
                "suggest",
                "instance",
                "bound",
                "remains"
            ]
        }
    },
    {
        "id": "d7e7b60d-40a6-4c51-952f-503d1f02507d",
        "title": "",
        "chunk_text": "A significant body of work has analyzed the empirical hardness of NP-complete problems, including Subset Sum and related variants such as k-Sum or Knapsack. Early investigations by [4] illustrated how “phase transitions” can yield exceptionally hard instance families, even though many randomly generated instances are surprisingly easy in practice.",
        "metadata": {
            "author": "",
            "keywords": [
                "Knapsack",
                "Subset",
                "Sum",
                "problems",
                "including",
                "significant",
                "body",
                "work",
                "analyzed",
                "empirical"
            ]
        }
    },
    {
        "id": "efe7e05a-dfde-4b04-8a54-789ad90879de",
        "title": "",
        "chunk_text": "Subsequent works [5, 6, 7] have shown that the density of Subset Sum instances (often measured by n/ log(max ai)) strongly influences typical-case runtimes. In particular, so-called “low-density” or “medium-density” inputs can often be solved faster than the classical 2n/2 meet-in-the- middle bound would suggest.",
        "metadata": {
            "author": "",
            "keywords": [
                "log",
                "Subset",
                "Sum",
                "Subsequent",
                "works",
                "instances",
                "max",
                "strongly",
                "runtimes",
                "so-called"
            ]
        }
    },
    {
        "id": "5231e25c-4c67-485a-ab23-3e8113cc3043",
        "title": "",
        "chunk_text": "Recent studies in cryptographic settings [8] further document large empirical speedups over the worst-case analysis, confirming that the actual difficulty of Subset Sum hinges dramatically on its structural properties and input distributions. Randomization can also help in practice. Several contributions have aimed to improve the time complexity for solving Subset Sum. In particular, randomized algorithms introduced in [8, 2] leverage the additive structure to reduce the effective search space.",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "Recent",
                "settings",
                "analysis",
                "confirming",
                "distributions",
                "studies",
                "cryptographic",
                "document"
            ]
        }
    },
    {
        "id": "cd4455c4-813f-4219-8753-2b5709d9c063",
        "title": "",
        "chunk_text": "Although these randomized methods often yield practical speedups, they introduce an element of uncertainty and do not guarantee an exact solution. These empirical findings reinforce our structure-sensitive perspective: they demonstrate that for many real or random-like instances (whether sparse or dense), the effective search space may be significantly smaller than 2n.",
        "metadata": {
            "author": "",
            "keywords": [
                "speedups",
                "solution",
                "randomized",
                "methods",
                "yield",
                "practical",
                "introduce",
                "element",
                "uncertainty",
                "guarantee"
            ]
        }
    },
    {
        "id": "b2a39ad9-177a-425d-808b-c67c78bcb29f",
        "title": "",
        "chunk_text": "Despite these advances, a universal method that deterministically exploits the combi- natorial structure of Subset Sum across all instances remains elusive. The challenge arises from the diversity of inputs: on the one hand, collision-rich (or small-range) instances can be tackled effectively by pseudopolynomial DP or meet-in-the-middle; on the other hand, sparse sets with minimal collisions may require different techniques (e.g., sparse convolution).",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "advances",
                "combi",
                "natorial",
                "elusive",
                "instances",
                "hand",
                "sparse",
                "universal"
            ]
        }
    },
    {
        "id": "c84fb6b7-27ed-49de-816c-c57fe05c3608",
        "title": "",
        "chunk_text": "Moreover, although results such as Freiman’s Theorem provide strong bounds for sets with small doubling constants, they do not extend uniformly to all instances. Motivated by these observations, we propose a structure-aware framework that leverages both the combinatorial properties of the enumeration process and the underlying additive combinatorics of the Subset Sum problem.",
        "metadata": {
            "author": "",
            "keywords": [
                "Freiman",
                "Theorem",
                "constants",
                "instances",
                "results",
                "provide",
                "strong",
                "bounds",
                "sets",
                "small"
            ]
        }
    },
    {
        "id": "f6ac13c0-e98b-42ad-814d-b674e4ef08ca",
        "title": "",
        "chunk_text": "Our approach centers on a novel unique subset sum enumerator that generates only the distinct subset sums, effectively parameterizing the runtime by the effective search space U. While traditional worst-case analyses focus on the 2n subsets, practical instance hardness is better captured by U.",
        "metadata": {
            "author": "",
            "keywords": [
                "effectively",
                "subset",
                "sum",
                "sums",
                "approach",
                "centers",
                "unique",
                "enumerator",
                "generates",
                "distinct"
            ]
        }
    },
    {
        "id": "1c1daeb0-f3e9-44e9-953d-1e8def9b7668",
        "title": "",
        "chunk_text": "By exploiting this insight, our adaptive, deterministic solver efficiently adjusts its behavior to the input’s structure—whether the instance is dense or sparse—thereby setting the stage for the detailed 3 exposition of our contributions in the subsequent sections. 2 Key Contributions In this work, we present a novel adaptive and structure-aware solver for the Subset Sum problem that dynamically adjusts to the instance structure.",
        "metadata": {
            "author": "",
            "keywords": [
                "contributions",
                "structure",
                "instance",
                "insight",
                "deterministic",
                "sparse",
                "detailed",
                "exposition",
                "sections",
                "adaptive"
            ]
        }
    },
    {
        "id": "661a54f0-2428-4ba4-8f8c-f5ff5a2fe261",
        "title": "",
        "chunk_text": "The key contributions of our approach are: Unique Subset Sums Enumeration: We introduce an enumeration strategy that generates each unique subset sum exactly once, eliminating redundant computations and dynamically pruning branches (§5). Double Meet-in-the-Middle Optimization: Our solver leverages a double meet-in- the-middle approach to efficiently combine solutions from two halves, avoiding expensive sorting and merging phases (§6).",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Unique",
                "Enumeration",
                "Optimization",
                "Sums",
                "sum",
                "eliminating",
                "branches",
                "approach",
                "key"
            ]
        }
    },
    {
        "id": "74b57cf9-1d75-4d55-a571-7e7000996bf8",
        "title": "",
        "chunk_text": "Combinatorial Tree Compression Optimization: We propose a lightweight struc- tural optimization that provably reduces the effective exponent in all cases, ensuring a strictly sub-2n/2 enumeration (§7). Anytime and Online Behavior: The solver supports incremental computation and allows seamless updates when new elements are introduced, making it suitable for real-time applications (§8).",
        "metadata": {
            "author": "",
            "keywords": [
                "enumeration",
                "Optimization",
                "Tree",
                "Compression",
                "Combinatorial",
                "struc",
                "tural",
                "cases",
                "ensuring",
                "strictly"
            ]
        }
    },
    {
        "id": "332fb440-0598-4871-9fad-ebf02f1f1b50",
        "title": "",
        "chunk_text": "Adaptivity to Instance Structure: We analyze how our method naturally adapts to structured instances—those with small doubling constants, high additive energy, or significant redundancy—often matching near-dynamic programming performance (§10- §15).",
        "metadata": {
            "author": "",
            "keywords": [
                "Structure",
                "Adaptivity",
                "constants",
                "high",
                "energy",
                "redundancy",
                "performance",
                "Instance",
                "instances",
                "analyze"
            ]
        }
    },
    {
        "id": "f6627179-f355-409c-b22f-42c96aef2a38",
        "title": "",
        "chunk_text": "Implications for Exponential-Time Algorithms: We discuss how our approach refines the complexity landscape of NP-complete problems by focusing on instance-specific hardness rather than worst-case complexity, and we outline directions for future work (§16). The remainder of the paper is organized as follows. In Section 3, we introduce the necessary background and notation. Section 4 reviews related work on Subset Sum and its variants.",
        "metadata": {
            "author": "",
            "keywords": [
                "Algorithms",
                "complexity",
                "Implications",
                "Section",
                "Exponential-Time",
                "discuss",
                "approach",
                "refines",
                "landscape",
                "NP-complete"
            ]
        }
    },
    {
        "id": "a00c4e02-2570-43e8-abe9-04167636482f",
        "title": "",
        "chunk_text": "In Section 5, we describe our unique subset sums enumerator, which forms the cornerstone of our approach. Section 6 details our double meet-in-the-middle optimization, and Section 7 presents our combinatorial tree compression technique to further reduce the enumeration cost. Next, Section 8 outlines our divide-and-conquer strategy that enables anytime operation, and Section 9 provides the additive-combinatorial underpinning to our method.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "enumerator",
                "approach",
                "describe",
                "unique",
                "subset",
                "sums",
                "forms",
                "cornerstone",
                "optimization"
            ]
        }
    },
    {
        "id": "a4342c23-7d5c-450c-9e67-994b4deaae17",
        "title": "",
        "chunk_text": "Section 10 then analyzes the adaptive time complexity and instance hardness. In Section 11, we present runtime experiments that demonstrate the adaptivity of our method, while Section 12 summarizes the overall runtime when all optimizations are combined. Section 13 then provides a comparative table of existing Subset Sum solvers, including our approach. Finally, Section 14 highlights the key insights and benefits of our framework, Section 15 briefly discusses the emerging dynamism of our method.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "hardness",
                "method",
                "analyzes",
                "adaptive",
                "time",
                "complexity",
                "instance",
                "runtime",
                "Subset"
            ]
        }
    },
    {
        "id": "57f3c810-20b0-441e-839b-c49d75e26757",
        "title": "",
        "chunk_text": "Section 16 explores broader implications and future directions, and Section 17 concludes the paper. 3 Preliminaries In this section, we introduce the basic concepts, notations, and parameters that form the foundation for our analysis and algorithm. 4 3.1 Basic Definitions Let S ⊂Z be a nonempty finite set of n integers. The classical Subset Sum Problem is defined as follows: ▶Definition 1 (Subset Sum Problem).",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "Sum",
                "explores",
                "directions",
                "concludes",
                "paper",
                "Subset",
                "Problem",
                "basic",
                "Preliminaries"
            ]
        }
    },
    {
        "id": "e984bb7b-a273-4ab3-bfc8-bfcfe577d802",
        "title": "",
        "chunk_text": "Given a set S of n integers and a target t satisfying 1 ≤t ≤ X x∈S x, determine whether there exists a subset S′ ⊆S such that X x∈S′ x = t. ▶Definition 2 (Asymptotic Notation). We use the notation e O(f(n)) to denote an upper bound that suppresses polylogarithmic factors. That is, a function g(n) is in e O(f(n)) if there exist constants c > 0 and k ≥0 such that for all sufficiently large n, g(n) ≤c f(n) · (log n)k. ▶Definition 3 (Density).",
        "metadata": {
            "author": "",
            "keywords": [
                "Definition",
                "satisfying",
                "determine",
                "Notation",
                "set",
                "integers",
                "target",
                "subset",
                "Asymptotic",
                "Density"
            ]
        }
    },
    {
        "id": "d8f323a1-881a-4a8b-af16-471d9e61b5d7",
        "title": "",
        "chunk_text": "We define the density of a Subset Sum instance S = {a1, a2, . . . , an} as d(S) = n log2(max S), which measures the ratio between the number of elements and the bit-length of the largest element. A higher density indicates that many elements are packed into a relatively small numerical range, often leading to more additive collisions and a smaller effective search space. ▶Definition 4 (k-Permutations and k-Subsets).",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "define",
                "instance",
                "density",
                "elements",
                "Definition",
                "max",
                "range",
                "space"
            ]
        }
    },
    {
        "id": "6c0bc72a-11b3-4c73-8c5b-335632c3a788",
        "title": "",
        "chunk_text": "For a set S, a k-permutation is an ordered selection of k distinct elements from S representing a partial solution in the enumeration process. When order is not significant, the same collection is referred to as a k-subset. In our algorithm, we generate k-permutations incrementally, and upon verifying that their aggregated sum is unique, we interpret them as k-subsets. ▶Definition 5 (Combinatorial Topology).",
        "metadata": {
            "author": "",
            "keywords": [
                "process",
                "set",
                "ordered",
                "selection",
                "distinct",
                "elements",
                "representing",
                "partial",
                "solution",
                "enumeration"
            ]
        }
    },
    {
        "id": "7fabe454-05bd-4f77-848c-a08fd3fb07ad",
        "title": "",
        "chunk_text": "The combinatorial topology of an instance S refers to the inherent structure of the combinatorial tree formed by all possible subsets of S. This topology is determined by the additive relationships within S, and remains fixed for a given static instance. It dictates how distinct k-permutations may converge to the same subset sum. 3.2 Unique Subset Sums Although there are 2n possible subsets of S, many of these subsets yield the same sum when the input set S exhibits additive structure.",
        "metadata": {
            "author": "",
            "keywords": [
                "combinatorial",
                "subsets",
                "topology",
                "instance",
                "subset",
                "refers",
                "inherent",
                "tree",
                "formed",
                "sum"
            ]
        }
    },
    {
        "id": "1aa34bb9-59ab-43ed-9c02-0e6e615a6589",
        "title": "",
        "chunk_text": "We define the set of all unique subset sums as: Σ(S) = ( X x∈S′ x : S′ ⊆S ) . The effective search space is then given by: U = |Σ(S)|. In many practical instances, U is significantly smaller than 2n, and our algorithm explicitly exploits this fact by parameterizing the running time in terms of U.",
        "metadata": {
            "author": "",
            "keywords": [
                "define",
                "set",
                "unique",
                "subset",
                "sums",
                "effective",
                "search",
                "space",
                "instances",
                "practical"
            ]
        }
    },
    {
        "id": "633391cd-ef1e-408a-a92c-4bc4f98b3e66",
        "title": "",
        "chunk_text": "5 3.3 Additive Structure Measures The following measures quantify the degree of additive structure in the set S: 3.3.0.1 Doubling Constant The doubling constant of S is defined by C = |S + S| |S| , where S + S = {a + b | a, b ∈S}. A low doubling constant indicates that many pairwise sums overlap, which in turn suggests that the effective number U of unique subset sums is reduced. 3.3.0.2 Additive Energy The additive energy E(S) is a measure of the number of additive collisions in S.",
        "metadata": {
            "author": "",
            "keywords": [
                "Additive",
                "Structure",
                "Doubling",
                "Constant",
                "Measures",
                "Energy",
                "quantify",
                "degree",
                "set",
                "defined"
            ]
        }
    },
    {
        "id": "e7f04a08-b1e8-497b-96ec-0ba0698ec312",
        "title": "",
        "chunk_text": "It is defined as the number of solutions in S to a + b = c + d, with a, b, c, d ∈S. High additive energy implies that many subsets yield the same sum, again reflecting a small U. 3.4 Extended Structural Parameters Beyond the doubling constant C and the additive energy E(S), we will also make use of three additional parameters to quantify the structure or redundancy in S. 3.4.0.1 Linearity Factor λ.",
        "metadata": {
            "author": "",
            "keywords": [
                "defined",
                "number",
                "solutions",
                "additive",
                "energy",
                "Parameters",
                "Extended",
                "Linearity",
                "Structural",
                "Factor"
            ]
        }
    },
    {
        "id": "d4305f86-1755-430e-bb37-3d7d80d95e96",
        "title": "",
        "chunk_text": "We define a linearity factor λ ∈[0, 1] to capture the extent to which the elements of S exhibit near-linear (e.g., arithmetic-progression-like) relationships. A smaller λ indicates that the set resembles a linear or nearly linear structure, often leading to higher collision rates among subset sums. 3.4.0.2 Clustering Factor γ. We define a clustering factor γ ∈[0, 1] to indicate how tightly the distinct sums in Σ(S) cluster.",
        "metadata": {
            "author": "",
            "keywords": [
                "relationships",
                "factor",
                "near-linear",
                "define",
                "Clustering",
                "linearity",
                "capture",
                "extent",
                "elements",
                "exhibit"
            ]
        }
    },
    {
        "id": "75c164ae-4a81-43f4-9a4d-8e41d4341998",
        "title": "",
        "chunk_text": "Smaller γ implies that many subset sums overlap or collide numerically, thus reducing the effective search space U. 3.4.0.3 Duplicate Measure δ. We let δ represent the effective number of duplicate elements in S. Repeated elements often cause multiple branches in our enumerator to converge onto the same partial sum, effectively merging those branches rather than keeping them distinct. Consequently, having more duplicates reduces the effective search space U.",
        "metadata": {
            "author": "",
            "keywords": [
                "effective",
                "Smaller",
                "numerically",
                "search",
                "space",
                "implies",
                "subset",
                "overlap",
                "collide",
                "reducing"
            ]
        }
    },
    {
        "id": "de59089c-6131-43d4-ab61-482173445e0c",
        "title": "",
        "chunk_text": "These three parameters (λ, γ, and δ) all capture different aspects of additive redundancy in S. In particular, they complement the more classical measures C and E(S) by providing additional insight into how and why the number of unique subset sums U might remain well below 2n. 6 Remark on “Input-Specific” Hardness.",
        "metadata": {
            "author": "",
            "keywords": [
                "parameters",
                "capture",
                "aspects",
                "additive",
                "redundancy",
                "Hardness",
                "Remark",
                "Input-Specific",
                "complement",
                "classical"
            ]
        }
    },
    {
        "id": "fe36013f-cc4d-47ec-867e-c21f8e483f69",
        "title": "",
        "chunk_text": "Throughout this paper, when we discuss “input-specific” or “structure-aware” complexity, we are referring to significant subfamilies of instances that exhibit certain structural properties (e.g., high collision rates, small doubling constants, or additive redundancy). These properties can cause the number of distinct subset sums, U = |Σ(S)|, to remain far below the naive 2n, making the instance practically easier than worst-case bounds would suggest.",
        "metadata": {
            "author": "",
            "keywords": [
                "input-specific",
                "structure-aware",
                "complexity",
                "paper",
                "discuss",
                "high",
                "rates",
                "small",
                "constants",
                "redundancy"
            ]
        }
    },
    {
        "id": "1e69c9b3-3a86-429b-b389-643b05c2b082",
        "title": "",
        "chunk_text": "This perspective does not contradict NP-completeness; it simply underscores that many real or structured inputs deviate markedly from the fully unstructured worst-case distribution, thus giving the solver room for substantial pruning. 3.5 Discussion In our analysis and experiments, we primarily measure an instance’s effective hardness by the number of unique subset sums U = |Σ(S)|.",
        "metadata": {
            "author": "",
            "keywords": [
                "NP-completeness",
                "distribution",
                "pruning",
                "perspective",
                "contradict",
                "simply",
                "underscores",
                "real",
                "structured",
                "inputs"
            ]
        }
    },
    {
        "id": "0a39511e-691d-49fb-b8a2-a33a89f3619a",
        "title": "",
        "chunk_text": "While the worst-case scenario for Subset Sum is governed by 2n possibilities, many practical instances exhibit significant additive structure—captured initially by the doubling constant C and additive energy E(S), but more finely parameterized by the linearity factor λ, clustering factor γ, and duplicate measure δ. Low Doubling / High Additive Energy: Such instances exhibit a substantial overlap among partial sums, thereby reducing U.",
        "metadata": {
            "author": "",
            "keywords": [
                "possibilities",
                "Subset",
                "additive",
                "energy",
                "factor",
                "doubling",
                "structure",
                "captured",
                "clustering",
                "instances"
            ]
        }
    },
    {
        "id": "b5fbb703-b433-45a2-8cbf-fbc44487e570",
        "title": "",
        "chunk_text": "Linearity Factor λ: If S is nearly linear or progression-like, many subsets collapse onto the same sums, again decreasing U. Clustering Factor γ: Even when S is not strictly linear, the subset sums can be tightly bunched numerically, indicating heavy overlap in the resulting sums. Duplicate Measure δ: Repeated elements cause multiple enumerator paths to merge onto the same partial sums—instead of branching off uniquely. Thus, duplicates can significantly reduce the overall search space U.",
        "metadata": {
            "author": "",
            "keywords": [
                "Factor",
                "sums",
                "Linearity",
                "progression-like",
                "linear",
                "collapse",
                "decreasing",
                "Repeated",
                "Measure",
                "subsets"
            ]
        }
    },
    {
        "id": "5f38dab1-d792-483b-9766-9e143e91fd2d",
        "title": "",
        "chunk_text": "Each of these parameters captures a different facet of additive redundancy. In essence, they all explain why, in many structured instances, the effective search space U can be dramatically smaller than 2n. Our algorithm takes advantage of these forms of redundancy by enumerating only distinct sums and pruning entire branches once collisions are detected. However, in unstructured instances, U remains close to 2n, so our method gracefully reverts to near-worst-case performance.",
        "metadata": {
            "author": "",
            "keywords": [
                "parameters",
                "captures",
                "facet",
                "additive",
                "instances",
                "redundancy",
                "essence",
                "performance",
                "explain",
                "structured"
            ]
        }
    },
    {
        "id": "0e708f08-20a3-4eff-9420-b249541419cf",
        "title": "",
        "chunk_text": "The subsequent sections detail how this perspective guides the design of our unique- subset-sums enumerator and our overall double meet-in-the-middle strategy. Implementation and Input Assumptions. For simplicity of exposition, we assume throughout that the input set S consists of nonnegative integers.",
        "metadata": {
            "author": "",
            "keywords": [
                "strategy",
                "unique",
                "subset-sums",
                "double",
                "subsequent",
                "sections",
                "detail",
                "perspective",
                "guides",
                "design"
            ]
        }
    },
    {
        "id": "a073fc5d-91d0-428f-ad87-35a4bf73f259",
        "title": "",
        "chunk_text": "Our enumerator and collision-pruning techniques readily apply even if S contains the integer 0 (this simply creates an immediate collision at sum 0 for any subset that chooses or omits 0), or if it contains negative integers (the memoization structure can store negative partial sums just as easily). In the worst case, storing all unique partial sums still requires e O(2n/2) space, although collision-rich inputs typically reduce memory usage sig- nificantly.",
        "metadata": {
            "author": "",
            "keywords": [
                "negative",
                "omits",
                "easily",
                "integer",
                "integers",
                "partial",
                "sums",
                "enumerator",
                "collision-pruning",
                "techniques"
            ]
        }
    },
    {
        "id": "75243b0a-74c0-4047-bd20-cc2a9743c893",
        "title": "",
        "chunk_text": "We leave fully parallel implementations and extended empirical tests—particularly for large (cryptographically sized) n—as future work. 7 4 Prior Work The Subset Sum problem has attracted extensive research due to its NP-completeness and practical significance. Early approaches, such as Bellman’s dynamic programming algorithm [9], offered pseudopolynomial-time solutions with time complexity O(n t) (where t is the target sum).",
        "metadata": {
            "author": "",
            "keywords": [
                "tests",
                "large",
                "cryptographically",
                "sized",
                "work",
                "Prior",
                "Sum",
                "leave",
                "fully",
                "parallel"
            ]
        }
    },
    {
        "id": "ab697911-4ec8-499c-a20c-8a3f17b0f94e",
        "title": "",
        "chunk_text": "Over time, several refined DP-based techniques have been proposed to mitigate this cost when t remains large. For instance, Koiliaris and Xu [10] introduced a faster pseudopolynomial-time algorithm running in e O(t√n) by leveraging FFT-based convolutions for partial-sum computations.",
        "metadata": {
            "author": "",
            "keywords": [
                "time",
                "large",
                "Koiliaris",
                "refined",
                "DP-based",
                "techniques",
                "proposed",
                "mitigate",
                "cost",
                "remains"
            ]
        }
    },
    {
        "id": "fbeddcb6-c140-46d0-b34e-4f72cbb7d9cd",
        "title": "",
        "chunk_text": "More recently, Bringmann [11] developed a near- linear pseudopolynomial algorithm for Subset Sum under specific parameter regimes, achieving further speedups in cases where the numeric range is not excessively large. Nevertheless, once t grows beyond a moderate threshold, these DP-based methods become infeasible in practice because their complexity remains tied to the numerical size of the target.",
        "metadata": {
            "author": "",
            "keywords": [
                "Bringmann",
                "Subset",
                "Sum",
                "recently",
                "developed",
                "linear",
                "regimes",
                "achieving",
                "large",
                "pseudopolynomial"
            ]
        }
    },
    {
        "id": "8282dbf1-91ae-4aed-bbcf-de70b71de4b1",
        "title": "",
        "chunk_text": "A major breakthrough was achieved with the meet-in-the-middle algorithm of Horowitz and Sahni [1], which achieves a worst-case time complexity of e O(2n/2) by splitting the input set into two parts and enumerating all subset sums for each half.",
        "metadata": {
            "author": "",
            "keywords": [
                "Sahni",
                "Horowitz",
                "algorithm",
                "half",
                "major",
                "breakthrough",
                "achieved",
                "achieves",
                "worst-case",
                "time"
            ]
        }
    },
    {
        "id": "84430246-4503-459f-bece-f71c32a7898f",
        "title": "",
        "chunk_text": "However, while this bound remains tight for pathological inputs, empirical evidence and more recent theoretical insights [11, 12, 13] reveal that practical Subset Sum hardness is more accurately governed by collisions in partial sums rather than the full 2n enumeration. Exploiting Additive Structure A key observation in recent work is that many inputs exhibit significant additive structure, which naturally leads to a high degree of such collisions.",
        "metadata": {
            "author": "",
            "keywords": [
                "enumeration",
                "Subset",
                "empirical",
                "insights",
                "reveal",
                "full",
                "Sum",
                "sums",
                "Additive",
                "Structure"
            ]
        }
    },
    {
        "id": "9cb03c36-6eb7-4fda-8e72-2200ce06a1cb",
        "title": "",
        "chunk_text": "This structure can be quantified by measures such as the doubling constant and additive energy. For instance, Freiman’s Theorem [14] guarantees that sets with small doubling can be embedded in a low-dimensional generalized arithmetic progression (GAP), implying that the effective search space U is dramatically smaller than 2n when the input is highly structured.",
        "metadata": {
            "author": "",
            "keywords": [
                "energy",
                "GAP",
                "doubling",
                "structure",
                "quantified",
                "measures",
                "constant",
                "additive",
                "Freiman",
                "Theorem"
            ]
        }
    },
    {
        "id": "29ff0d5f-fa33-4092-a0c0-dbf1b6c76b90",
        "title": "",
        "chunk_text": "Randomized Algorithms Randomized approaches [8, 2] have been proposed to exploit additive structure by using collision-based pruning techniques. These algorithms navigate the combinatorial tree of subset sums probabilistically, effectively reducing the number of distinct paths that must be examined. While these methods often yield substantial practical speedups, they inherently introduce uncertainty and do not always guarantee an exact solution.",
        "metadata": {
            "author": "",
            "keywords": [
                "Randomized",
                "approaches",
                "techniques",
                "Algorithms",
                "proposed",
                "exploit",
                "additive",
                "structure",
                "collision-based",
                "pruning"
            ]
        }
    },
    {
        "id": "e26e2122-9dd3-4687-bb17-d9d98ef4f846",
        "title": "",
        "chunk_text": "Challenges and the Need for Determinism Despite the improvements provided by randomized techniques, these methods generally do not extend well to all instances, especially sparse inputs where collisions are infrequent. Moreover, existing deterministic methods (such as the classical meet-in-the-middle approach) do not fully exploit the underlying additive structure of the input.",
        "metadata": {
            "author": "",
            "keywords": [
                "Determinism",
                "Challenges",
                "techniques",
                "instances",
                "infrequent",
                "methods",
                "improvements",
                "provided",
                "randomized",
                "generally"
            ]
        }
    },
    {
        "id": "302b4175-ef9f-4708-8da1-758412013b53",
        "title": "",
        "chunk_text": "There remains a gap in the literature for a universal, deterministic method that adapts to the input’s additive properties. Relation to Instance Complexity An important theoretical angle for analyzing how NP-complete problems can be signifi- cantly easier on “structured” inputs is the notion of instance complexity studied by Orponen, 8 Ko, Schöning, and Watanabe [15].",
        "metadata": {
            "author": "",
            "keywords": [
                "universal",
                "deterministic",
                "properties",
                "Schöning",
                "Instance",
                "Complexity",
                "remains",
                "gap",
                "literature",
                "method"
            ]
        }
    },
    {
        "id": "ba0277f1-1dc8-45b7-9dc6-f041514cd4c0",
        "title": "",
        "chunk_text": "Informally, instance complexity measures the size of a “special-case program” that decides whether a particular input x belongs to the language A, within a given time bound, while never misclassifying any other inputs (it may answer “don’t know” on them). They prove that although NP-hard sets still have infinitely many “intrinsically hard” instances, many classes of instances admit specialized programs of lower complexity.",
        "metadata": {
            "author": "",
            "keywords": [
                "Informally",
                "special-case",
                "bound",
                "answer",
                "complexity",
                "measures",
                "size",
                "decides",
                "belongs",
                "language"
            ]
        }
    },
    {
        "id": "d7d3feb5-3994-402b-9af4-5f77433da6c6",
        "title": "",
        "chunk_text": "Their results help explain why algorithms exploiting collisions or other structural properties can solve large subfamilies of Subset Sum instances much faster than the worst-case 2n/2. Our Contribution Recent research [11, 2] has indicated that parameterizing Subset Sum in terms of U (or related structural measures) can yield exponential speedups in practical cases. However, prior work has largely relied on randomized methods or specific assumptions on the input structure.",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "worst-case",
                "structural",
                "results",
                "explain",
                "algorithms",
                "exploiting",
                "collisions",
                "properties"
            ]
        }
    },
    {
        "id": "076d56c0-7508-4fde-babb-60758262a088",
        "title": "",
        "chunk_text": "Our work fills this gap by introducing a deterministic, structure-aware framework that leverages the effective search space U as a parameter, thereby providing a unified approach that adapts seamlessly to both dense and sparse instances. The collision-driven pruning in our unique subset-sums enumerator does more than simply reduce enumeration overhead; it also offers a real-time lens into the structure of the input.",
        "metadata": {
            "author": "",
            "keywords": [
                "deterministic",
                "structure-aware",
                "parameter",
                "instances",
                "work",
                "fills",
                "gap",
                "introducing",
                "framework",
                "leverages"
            ]
        }
    },
    {
        "id": "fea0d95d-71e5-4b4f-bbdb-9840cc1cb8d8",
        "title": "",
        "chunk_text": "Specifically, the enumerator’s memoization table tracks how often newly formed partial sums coincide with previously encountered sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "Specifically",
                "sums",
                "enumerator",
                "memoization",
                "table",
                "tracks",
                "newly",
                "formed",
                "partial",
                "coincide"
            ]
        }
    },
    {
        "id": "538e8e21-aeda-47bc-8b6c-83cd8800891d",
        "title": "",
        "chunk_text": "When collisions appear frequently and early, this strongly indicates that the input set S exhibits nontrivial additive or redundant structure, for example: A small doubling constant (|S + S| ≤C|S|), High additive energy (a + b = c + d collisions), Clustered or near-linear arrangements of elements, Repetitions or duplicates that force multiple branches onto the same sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "High",
                "Clustered",
                "Repetitions",
                "collisions",
                "additive",
                "early",
                "structure",
                "constant",
                "energy",
                "elements"
            ]
        }
    },
    {
        "id": "ca659117-c464-42a8-bb81-af07a2feed3c",
        "title": "",
        "chunk_text": "Without any additional preprocessing, the enumerator naturally adapts to collisions as they occur, effectively exploiting the input’s underlying structure (whether dense, sparse, or in-between) and automatically adjusting its work to reflect the true hardness U = |Σ(S)|. 5 Unique Subset Sums Enumerator A naive solution to the Subset Sum problem would exhaustively enumerate all 2n subsets of S and check whether any of them sum to the target t.",
        "metadata": {
            "author": "",
            "keywords": [
                "sparse",
                "enumerator",
                "preprocessing",
                "occur",
                "effectively",
                "structure",
                "dense",
                "in-between",
                "Subset",
                "Sum"
            ]
        }
    },
    {
        "id": "4b2da0ff-3972-4f28-9223-7f55d39da697",
        "title": "",
        "chunk_text": "In practice, however, when the input set S exhibits significant additive structure, many of these 2n subsets produce redundant sums. The true challenge lies in generating the unique subset sums—that is, the effective search space of distinct sums defined by Σ(S) = ( X x∈S′ x : S′ ⊆S ) . We denote the size of this set by U = |Σ(S)|. In many instances, U is dramatically smaller than 2n.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "practice",
                "structure",
                "set",
                "input",
                "exhibits",
                "significant",
                "additive",
                "produce",
                "redundant"
            ]
        }
    },
    {
        "id": "7feb390c-f7ad-49ea-a276-612a2621fded",
        "title": "",
        "chunk_text": "9 Our novel enumeration model provides access to all generated data (i.e., the subsets of S and their sums) throughout the process. To implement the enumerator, we designed a novel permutation-generating algorithm. Conventional permutation generators output one full permutation per step (for example, by swapping two elements from the previous permutation). In contrast, our approach generates all possible permutations of S one column at a time (i.e., left-to-right).",
        "metadata": {
            "author": "",
            "keywords": [
                "data",
                "sums",
                "process",
                "enumeration",
                "model",
                "access",
                "generated",
                "subsets",
                "permutation",
                "enumerator"
            ]
        }
    },
    {
        "id": "20d54665-7845-45b4-aa11-681bca687dab",
        "title": "",
        "chunk_text": "At each step, the algorithm emits the k-permutations (or k-subsets) of increasing cardinality according to the following process: Initial step: Start with an INPUT list containing the empty k-permutation, {}. For each step: Extend each entry in the INPUT by one column by appending every element from S not already included, and add these expanded k-permutations to the OUTPUT. Once all entries in INPUT have been processed, set INPUT ←OUTPUT and repeat until no new k-permutations are generated.",
        "metadata": {
            "author": "",
            "keywords": [
                "INPUT",
                "step",
                "Initial",
                "Start",
                "OUTPUT",
                "k-permutations",
                "k-subsets",
                "process",
                "algorithm",
                "emits"
            ]
        }
    },
    {
        "id": "4bfa80d7-540e-4597-97aa-a7a1807bd2ac",
        "title": "",
        "chunk_text": "For example, consider the set S = {1, 2, 3, 4, 5}: Column Expansion Sub-process 1 Input = {{}} =⇒ Output = {{1}, {2}, {3}, {4}, {5}} Input ←Output ——— Column Expansion Sub-process 2 Input = {{1}, {2}, {3}, {4}, {5}} =⇒ Output = {{1, 2}, {1, 3}, {1, 4}, {1, 5}, {2, 1}, {2, 3}, . .",
        "metadata": {
            "author": "",
            "keywords": [
                "Input",
                "Output",
                "Column",
                "Sub-process",
                "Expansion",
                "set"
            ]
        }
    },
    {
        "id": "9f4e8ffb-7221-4a1b-9afe-daad7f5e7095",
        "title": "",
        "chunk_text": ".} Input ←Output ——— Column Expansion Sub-process N (up to i ≤n 2 ) Input = Output[i −1] =⇒ Output[i] Since permutations in isolation do not directly solve the Subset Sum problem, our algorithm tracks the sum of each expanded k-permutation during the column expansion process. Before adding an expanded k-permutation to the OUTPUT, the algorithm checks whether its sum has already been encountered in the memoization table; if so, the k- permutation is pruned.",
        "metadata": {
            "author": "",
            "keywords": [
                "Input",
                "Output",
                "Column",
                "Expansion",
                "Sum",
                "Sub-process",
                "Subset",
                "expanded",
                "k-permutation",
                "algorithm"
            ]
        }
    },
    {
        "id": "5cfa49b4-a998-43bb-98e9-8eafeb265994",
        "title": "",
        "chunk_text": "This pruning mechanism eliminates enumerating redundant branches in the combinatorial tree, yielding an optimal algorithm for generating the unique subset sums from S and, hence, directly determining U. Once this process completes, all possible k-permutations for k ≤n 2 are generated. We restrict the enumeration to k ≤n 2 because any subset with more than n 2 elements can be obtained by computing its complement with respect to S.",
        "metadata": {
            "author": "",
            "keywords": [
                "tree",
                "yielding",
                "directly",
                "pruning",
                "mechanism",
                "eliminates",
                "enumerating",
                "redundant",
                "branches",
                "combinatorial"
            ]
        }
    },
    {
        "id": "228e3ee1-c49f-442a-af27-e26569faaa1c",
        "title": "",
        "chunk_text": "This strategy eliminates redundant enumeration and significantly reduces both compu- tational overhead and memory usage, while ensuring that all unique subset sums—i.e., all elements of Σ(S) (and hence the effective parameter U) are captured. 10 For the complete pseudocode of our unique subsets sum enumerator algorithm, please refer to Appendix A.",
        "metadata": {
            "author": "",
            "keywords": [
                "compu",
                "tational",
                "usage",
                "captured",
                "unique",
                "strategy",
                "eliminates",
                "redundant",
                "enumeration",
                "significantly"
            ]
        }
    },
    {
        "id": "7d929d87-c5da-4343-ba37-ae5744da634c",
        "title": "",
        "chunk_text": "5.1 k-Permutations and k-Subsets and the Combinatorial Tree Structure In this paper, we use the terms k-permutation and k-subset interchangeably to refer to the same entity, with the interpretation depending on the phase of the algorithm: When an entity appears in the INPUT or OUTPUT lists during any column expansion sub-process, it is treated as a k-subset.",
        "metadata": {
            "author": "",
            "keywords": [
                "k-subset",
                "entity",
                "Combinatorial",
                "Tree",
                "Structure",
                "INPUT",
                "OUTPUT",
                "paper",
                "algorithm",
                "sub-process"
            ]
        }
    },
    {
        "id": "b6f190c6-abe2-4f60-beb9-56cb3aaed2b4",
        "title": "",
        "chunk_text": "In this context, it represents a subset of k elements that sums to a unique value and has passed the Unique Subset Sum constraint, which is the primary focus of our solver. When the entity serves as a prefix for generating (k + 1)-permutations during the column expansion, it is viewed as a k-permutation. Just before validation against the Unique Subset Sum constraint, the expanded (k + 1)- permutation is reinterpreted as a k-subset; if it satisfies the constraint, it is then added to the OUTPUT.",
        "metadata": {
            "author": "",
            "keywords": [
                "subset",
                "unique",
                "constraint",
                "Sum",
                "context",
                "solver",
                "represents",
                "elements",
                "passed",
                "primary"
            ]
        }
    },
    {
        "id": "558b6904-b2d1-4156-95fc-eb5ca5dafd58",
        "title": "",
        "chunk_text": "This duality clarifies the logic behind both the enumerator and the solver. k-permutations allow us to efficiently traverse and query the combinatorial tree structure during candidate generation, while k-subsets provide the precise representations needed to verify unique subset sums. This dual treatment is central to our approach, ensuring that the enumeration process remains both dynamic and efficient. 5.2 Seeding support An important feature of our enumerator is its support for seeding.",
        "metadata": {
            "author": "",
            "keywords": [
                "solver",
                "duality",
                "clarifies",
                "logic",
                "enumerator",
                "Seeding",
                "support",
                "k-permutations",
                "generation",
                "sums"
            ]
        }
    },
    {
        "id": "172e700e-cb4c-4a87-a273-120fc33c02c8",
        "title": "",
        "chunk_text": "If the memoization table is pre-populated with unique subset sums (i.e., elements of Σ(S)) and their corresponding k-subsets, the enumerator can seamlessly resume processing from that point. This capability allows intermediate results from previous cycles to be preserved and re-utilized, thereby reducing redundant computation and enabling an anytime (or incremental) behavior.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "elements",
                "k-subsets",
                "point",
                "memoization",
                "table",
                "pre-populated",
                "unique",
                "subset",
                "enumerator"
            ]
        }
    },
    {
        "id": "78f0506a-fd5d-4bda-9485-e69908301633",
        "title": "",
        "chunk_text": "5.3 Convolution-Like Behavior in k-Permutation Enumeration Although our unique subset sum enumerator does not perform convolution in the strict numerical sense (e.g., via the FFT), it exhibits behavior that is conceptually analogous to convolution. Traditional convolution combines two sequences A and B to produce a new sequence: A + B = {a + b | a ∈A, b ∈B}. In our approach, the enumeration process generates k-permutations, which represent all possible orderings of k-subsets of the input set S.",
        "metadata": {
            "author": "",
            "keywords": [
                "Behavior",
                "FFT",
                "convolution",
                "Enumeration",
                "Convolution-Like",
                "sense",
                "unique",
                "subset",
                "sum",
                "enumerator"
            ]
        }
    },
    {
        "id": "9d9328bd-3487-4dd9-8c20-73ed00eeb443",
        "title": "",
        "chunk_text": "This process has several key characteristics that resemble convolution: Aggregation of Partial Sums: As the algorithm extends partial k-permutations by appending new elements, it aggregates the resulting partial sums in a manner analogous to convolution summing overlapping portions of signals. This aggregation naturally leads to the identification of collisions—i.e., multiple k-permutations yielding the same subset sum.",
        "metadata": {
            "author": "",
            "keywords": [
                "Partial",
                "convolution",
                "Aggregation",
                "Sums",
                "elements",
                "signals",
                "k-permutations",
                "process",
                "key",
                "characteristics"
            ]
        }
    },
    {
        "id": "31aae582-5f7a-4bb3-8b76-36333ab18b57",
        "title": "",
        "chunk_text": "11 Propagation Through the Combinatorial Tree: Different orderings of the elements propagate the partial sums through various branches of the combinatorial tree. Im- portantly, many distinct paths converge on the same k-subset sum, since the order of elements does not affect the numerical total. This convergence mirrors how convolution combines contributions from shifted versions of a sequence, ensuring that all possible paths leading to the same sum are effectively merged.",
        "metadata": {
            "author": "",
            "keywords": [
                "Tree",
                "Combinatorial",
                "Propagation",
                "elements",
                "sum",
                "orderings",
                "propagate",
                "partial",
                "branches",
                "paths"
            ]
        }
    },
    {
        "id": "eb13dfd0-251f-438e-b815-0a99a93a34f2",
        "title": "",
        "chunk_text": "Implicit Combination of Branches: The self-similarity and structural properties of k-permutations ensure that once a particular subset sum is computed, further redundant branches that would yield the same sum are pruned. Much like convolution naturally consolidates overlapping components, our method consolidates identical subset sums, thereby reducing the effective search space.",
        "metadata": {
            "author": "",
            "keywords": [
                "Branches",
                "Combination",
                "Implicit",
                "computed",
                "pruned",
                "subset",
                "sum",
                "self-similarity",
                "structural",
                "properties"
            ]
        }
    },
    {
        "id": "7b4eeac7-0cb3-4abd-b68c-e5e70613bbbc",
        "title": "",
        "chunk_text": "Thus, while our algorithm does not explicitly implement FFT-based numerical convolution, it performs a topological convolution whose effects are expressed numerically: as the k- permutation enumeration process sweeps through the combinatorial space, it aggregates partial solutions, causing many distinct orderings to converge to the same unique subset sum.",
        "metadata": {
            "author": "",
            "keywords": [
                "convolution",
                "numerically",
                "permutation",
                "space",
                "solutions",
                "causing",
                "sum",
                "algorithm",
                "explicitly",
                "implement"
            ]
        }
    },
    {
        "id": "9351c61e-2df6-4fc7-a913-5069619bfd07",
        "title": "",
        "chunk_text": "5.4 A 4-Column Litmus Test for Instance Hardness and Splitting One practical way to gauge how “collision-rich” a Subset Sum instance is—and thus how large or small its effective search space U might be—is to run our enumerator only up to the 4th column. Concretely, this amounts to enumerating all subsets of size ≤4 from some subset of the input and measuring how often their sums collide. Polynomial-Time Approximation of U.",
        "metadata": {
            "author": "",
            "keywords": [
                "Instance",
                "Litmus",
                "Test",
                "Hardness",
                "Splitting",
                "collision-rich",
                "column",
                "Subset",
                "practical",
                "gauge"
            ]
        }
    },
    {
        "id": "bd6adb6d-f7f5-4723-a675-02357d054227",
        "title": "",
        "chunk_text": "If we apply this 4-column test to one half of the input (of size n/2), we must enumerate 4 X k=0 \u0012n/2 k \u0013 ≈ O \u0000(n/2)4\u0001 = O(n4). Hence, it is a polynomial-time procedure. The fraction of collisions among these 4-subsets can serve as a proxy for how dense or structured the instance is. If collisions already appear frequently in subsets as small as size 4, then the entire half will likely have a smaller U overall. If collisions remain rare, that half may be near-worst-case. Per-Split vs.",
        "metadata": {
            "author": "",
            "keywords": [
                "test",
                "input",
                "enumerate",
                "collisions",
                "half",
                "apply",
                "size",
                "procedure",
                "polynomial-time",
                "rare"
            ]
        }
    },
    {
        "id": "170fe6b7-08f7-4b67-90e1-a468ef27e798",
        "title": "",
        "chunk_text": "Whole-Set Testing. In a meet-in-the-middle setting, it is natural to apply this partial expansion on each split ℓ0 and ℓ1 separately to approximate U0 and U1 (the collision rate in each half). Alternatively, one can: (i) Test on the entire set of size n: enumerating 4-subsets up to \u0000n 4 \u0001 , still O(n4), to measure collisions globally before deciding how to split.",
        "metadata": {
            "author": "",
            "keywords": [
                "Testing",
                "Whole-Set",
                "split",
                "Test",
                "setting",
                "separately",
                "approximate",
                "half",
                "Alternatively",
                "enumerating"
            ]
        }
    },
    {
        "id": "cd922ec6-9c8e-44e1-9647-2d2b70dc211f",
        "title": "",
        "chunk_text": "(ii) Balance or refine the splits: if one half shows a much higher collision rate than the other, you might reassign a few elements from the sparser half to the denser half to improve overall performance of the double meet-in-the-middle. Thus, the 4-column test not only approximates the instance’s hardness in polynomial time but can also guide how we partition or balance the input for the main solver.",
        "metadata": {
            "author": "",
            "keywords": [
                "half",
                "Balance",
                "splits",
                "double",
                "refine",
                "shows",
                "higher",
                "collision",
                "rate",
                "reassign"
            ]
        }
    },
    {
        "id": "a27af826-2056-4e1c-aeb2-e6e67c8a468f",
        "title": "",
        "chunk_text": "It is a light preliminary step (only O(n4) time) that can significantly improve the final exponential search by exploiting early collision information. 12 5.5 Correctness and Completeness of Enumerating Unique Sums ▶Theorem 6 (Enumerator Correctness). Let E be the enumerator described in Section 5, which expands partial solutions one element at a time, pruning a branch whenever the new sum collides with a previously recorded sum. Then: 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "Correctness",
                "step",
                "information",
                "time",
                "Theorem",
                "Enumerator",
                "light",
                "preliminary",
                "significantly",
                "improve"
            ]
        }
    },
    {
        "id": "fd202a21-accc-4308-89e9-423d821d76ff",
        "title": "",
        "chunk_text": "(Completeness) For every subset S′ ⊆S, its sum P x∈S′ x eventually appears in the enumerator’s memoization table (i.e., no unique sum is ever missed). 2. (Uniqueness) Each distinct sum in Σ(S) is generated exactly once (the enumerator never duplicates the same sum). Proof. We prove each property separately. (1) Completeness. Let S′ = {xi1, xi2, . . . , xik} be any subset of S, written such that i1 < i2 < · · · < ik.",
        "metadata": {
            "author": "",
            "keywords": [
                "Completeness",
                "sum",
                "Uniqueness",
                "enumerator",
                "table",
                "missed",
                "eventually",
                "memoization",
                "unique",
                "subset"
            ]
        }
    },
    {
        "id": "9adaac3f-7c47-41e8-9600-33b12f69a65d",
        "title": "",
        "chunk_text": "We argue by induction on k that the enumerator generates the partial sum P x∈S′ x at some stage. Base Case (k = 0): If S′ = ∅, then its sum is 0. By construction, the enumerator initializes with the empty subset (sum 0) in its memoization structure. Inductive Step: Suppose for all subsets of size at most k, the enumerator eventually includes their sums. Consider a subset S′ of size k + 1 with elements indexed in ascending order.",
        "metadata": {
            "author": "",
            "keywords": [
                "enumerator",
                "stage",
                "sum",
                "Case",
                "argue",
                "induction",
                "generates",
                "partial",
                "size",
                "subset"
            ]
        }
    },
    {
        "id": "d1fed944-be99-4c7a-a98b-6498e156c96b",
        "title": "",
        "chunk_text": "Let S′′ = S′ \\ {xik+1} be the subset formed by removing the last element in that ordering. Then S′′ is of size k and by the inductive hypothesis, P x∈S′′ x is already in the enumerator’s memo. When the algorithm attempts to expand S′′ by adding new elements, it eventually considers adding xik+1. If at that moment the enumerator sees that P x∈S′′ x + xik+1 is not yet in the memo, it inserts it, thereby generating the sum of S′.",
        "metadata": {
            "author": "",
            "keywords": [
                "xik",
                "ordering",
                "subset",
                "formed",
                "removing",
                "memo",
                "enumerator",
                "adding",
                "element",
                "elements"
            ]
        }
    },
    {
        "id": "e2c4c965-3862-4915-99a6-252cd9fa9508",
        "title": "",
        "chunk_text": "Even if the enumerator visits other elements prior to xik+1, the eventual column-by-column (or step-by-step) expansion ensures that every unchecked element is tried. Collisions only prune repeated sums, never blocking a new one. Thus, by induction, every subset sum P x∈S′ x eventually appears. (2) Uniqueness. Assume for contradiction that the enumerator inserts the same sum s twice into its memoization table.",
        "metadata": {
            "author": "",
            "keywords": [
                "xik",
                "eventual",
                "expansion",
                "visits",
                "prior",
                "ensures",
                "unchecked",
                "elements",
                "element",
                "enumerator"
            ]
        }
    },
    {
        "id": "9d364982-402d-4276-8674-0a3a9d9ffcee",
        "title": "",
        "chunk_text": "Let A and B be two different subsets with P A = P B = s, and suppose they appear at different times in the enumerator’s process. Without loss of generality, let A be discovered first. The enumerator maintains a hash set or table storing all sums seen so far. Hence, by the time it considers generating B, it checks whether s is already in the table. Since A already placed s there, B’s creation would be pruned (the branch is skipped). This contradicts the assumption that B was inserted.",
        "metadata": {
            "author": "",
            "keywords": [
                "process",
                "enumerator",
                "subsets",
                "suppose",
                "table",
                "generality",
                "times",
                "time",
                "loss",
                "discovered"
            ]
        }
    },
    {
        "id": "a95b6ea8-d8ba-4e01-95ad-68079bdbf7b1",
        "title": "",
        "chunk_text": "Therefore, each distinct sum s enters the memo exactly once. Combining the two parts yields that the enumerator collects all unique sums in Σ(S) exactly once. ◀ 5.6 Bounded-Size Enumeration for Complements ▶Lemma 7 (Half-Subset Enumeration Preserves Completeness). Suppose the enumerator is restricted to enumerating only subsets of size at most ⌊n/4⌋in one half of the input (with n/2 13 elements). Let Umax be the number of unique sums enumerated by that half.",
        "metadata": {
            "author": "",
            "keywords": [
                "Enumeration",
                "sums",
                "distinct",
                "enters",
                "memo",
                "enumerator",
                "Lemma",
                "unique",
                "Complements",
                "Completeness"
            ]
        }
    },
    {
        "id": "6f6bd935-9adc-4bf3-9065-a03036087931",
        "title": "",
        "chunk_text": "Then for any subset S′ in that half of size > n/4, we can represent S′ as the complement of some smaller subset. Hence the enumerator (plus complement logic) still captures all sums P x∈S′ x. Proof Sketch. Let ℓbe the list of n/2 elements in one half. If |S′| > n 4 , its complement S∗⊂ℓhas size |ℓ| −|S′| < n 4 . By definition, P x∈S′ x + P y∈S∗y = P z∈ℓz.",
        "metadata": {
            "author": "",
            "keywords": [
                "subset",
                "complement",
                "size",
                "represent",
                "smaller",
                "half",
                "Sketch",
                "enumerator",
                "logic",
                "captures"
            ]
        }
    },
    {
        "id": "89bd8492-f4fc-47de-a5c1-0830ff2c35fc",
        "title": "",
        "chunk_text": "Once the enumerator records the sum P y∈S∗y, we store a pointer (or a simple record) that S∗’s complement yields the sum of S′ which is P x∈S′ x. Thus, no sums arising from subsets larger than n/4 are missed. ◀ 5.7 Implementation Invariants In a practical implementation, we typically maintain: A queue/stack of partial k-subsets (or k-permutations), A memoization hash table, Memo, mapping each sum σ to (i) a boolean or usage counter, and (ii) a pointer to the k-subset that generated σ.",
        "metadata": {
            "author": "",
            "keywords": [
                "sum",
                "pointer",
                "records",
                "record",
                "enumerator",
                "store",
                "simple",
                "complement",
                "yields",
                "Implementation"
            ]
        }
    },
    {
        "id": "3dfa3560-cff6-4873-938a-a8b5dd388530",
        "title": "",
        "chunk_text": "One can show that throughout the enumeration, for each partial sum σ in Memo: σ = X x∈X x for some unique subset X ⊆S. Whenever we consider adding a new element x to X, we do a hash-table lookup on σ + x. This ensures collisions are detected immediately. Because a previously generated σ + x can never be inserted again, the data structure remains consistent until the entire half-subset search finishes. — Remarks.",
        "metadata": {
            "author": "",
            "keywords": [
                "Memo",
                "enumeration",
                "show",
                "partial",
                "sum",
                "unique",
                "subset",
                "Remarks",
                "adding",
                "element"
            ]
        }
    },
    {
        "id": "d43ba89d-13de-4467-8758-a65e00b8834f",
        "title": "",
        "chunk_text": "These results ensure that the column-by-column approach is both correct and never repeats the same sum. Combined with meet-in-the-middle and complement usage (see Sections 6–7), this gives a time complexity of e O(2n/2) in the worst case, but can be significantly lower when collisions are frequent.",
        "metadata": {
            "author": "",
            "keywords": [
                "approach",
                "sum",
                "results",
                "ensure",
                "correct",
                "repeats",
                "Sections",
                "Combined",
                "usage",
                "case"
            ]
        }
    },
    {
        "id": "c826933f-2c05-4f8f-94c2-5e5bb6339b3f",
        "title": "",
        "chunk_text": "5.8 Brief Note on BFS-Based Approaches Although our method’s column-by-column enumeration might appear “BFS-like” at first glance, we stress that it diverges from standard BFS-based subset enumerations in critical ways. Classical BFS methods—including those used in certain meet-in-the-middle merges such as Schroeppel–Shamir [23]—expand subsets in discrete levels (e.g., all subsets of size k before moving to size k + 1).",
        "metadata": {
            "author": "",
            "keywords": [
                "BFS-like",
                "Note",
                "Approaches",
                "BFS-Based",
                "glance",
                "Shamir",
                "Schroeppel",
                "stress",
                "diverges",
                "standard"
            ]
        }
    },
    {
        "id": "0d9047d1-d74e-4d6c-9dee-a0ded63b5a7c",
        "title": "",
        "chunk_text": "Even when BFS integrates real-time collision checks (using hash sets to prune duplicates), it still follows a layer-oriented expansion that carries all partial sums from that layer in memory before proceeding. Our Enumerator vs. BFS. Column-by-Column vs. Layer-by-Layer: Rather than enumerating by subset cardinalities, our solver expands “one new element at a time” in a topological manner.",
        "metadata": {
            "author": "",
            "keywords": [
                "BFS",
                "checks",
                "duplicates",
                "proceeding",
                "integrates",
                "real-time",
                "collision",
                "hash",
                "sets",
                "prune"
            ]
        }
    },
    {
        "id": "0d5d39e7-afc0-4235-b52b-aef35b39eab2",
        "title": "",
        "chunk_text": "The enumerator prunes each branch immediately upon collision without waiting for a layer to complete or merging to occur in bulk. This avoids ephemeral duplicate states that BFS might generate in a single layer. 14 Adaptive Scheduling and Seeding: We allow partial k-permutations to be deferred to a future cycle if their expansions appear unpromising (see §8). This fine-grained scheduling is not commonly present in BFS-based merges, where expansions are typically uniform within a layer.",
        "metadata": {
            "author": "",
            "keywords": [
                "layer",
                "bulk",
                "enumerator",
                "prunes",
                "branch",
                "immediately",
                "collision",
                "waiting",
                "complete",
                "merging"
            ]
        }
    },
    {
        "id": "81054d60-5f7b-488b-b442-13a43426d44a",
        "title": "",
        "chunk_text": "Numerical and Topological Convolution: Our enumerator includes a unique combination of lightweight DP checks, forced collisions, and the generation of k-permutations in a manner that “collapses” repeated sums much earlier. By contrast, BFS enumerations typically store or process many partial sums in a given level, possibly discarding duplicates only after a layer-wide pass.",
        "metadata": {
            "author": "",
            "keywords": [
                "Convolution",
                "collapses",
                "Topological",
                "Numerical",
                "checks",
                "forced",
                "collisions",
                "repeated",
                "earlier",
                "sums"
            ]
        }
    },
    {
        "id": "53d69d87-2e24-4488-b227-b90775b9bfa9",
        "title": "",
        "chunk_text": "Although some BFS-based techniques can prune collisions on the fly too, the core dataflow in our method remains distinct: it intermixes meet-in-the-middle logic, dynamic pruning, and an anytime scheduling of expansions. Hence, we do not directly compare run times against a purely BFS-based subset enumeration—the algorithmic philosophies differ substantially.",
        "metadata": {
            "author": "",
            "keywords": [
                "logic",
                "distinct",
                "intermixes",
                "dynamic",
                "pruning",
                "expansions",
                "BFS-based",
                "techniques",
                "prune",
                "collisions"
            ]
        }
    },
    {
        "id": "40e4a636-5731-45e1-b77d-f477ee1eac08",
        "title": "",
        "chunk_text": "(As an analogy, BFS enumerates by cardinality layers, while we expand by columns and partial-sum uniqueness in a topological-numerical tree.) Why a Head-to-Head BFS Comparison is Nontrivial. We acknowledge that, in principle, one might code a BFS approach to attempt similar collision checks. Yet ensuring that BFS can replicate our incremental scheduling, column expansions, combinatorial-tree-short-circuits, and real-time integration with meet-in-the- middle is not straightforward.",
        "metadata": {
            "author": "",
            "keywords": [
                "BFS",
                "analogy",
                "layers",
                "tree",
                "Nontrivial",
                "enumerates",
                "cardinality",
                "expand",
                "partial-sum",
                "uniqueness"
            ]
        }
    },
    {
        "id": "9b5f908a-763f-438f-b312-8ed389e13565",
        "title": "",
        "chunk_text": "Introducing these features into a level-based BFS would effectively transform it into a different enumeration schema—one close to our method. Therefore, while some BFS improvements (e.g., Schroeppel–Shamir’s memory optimizations) overlap with our goals, the two techniques differ enough in design principle that we focus primarily on standard MIM, DP, and additive-structure solvers for our main comparisons (§13).",
        "metadata": {
            "author": "",
            "keywords": [
                "BFS",
                "Introducing",
                "schema",
                "method",
                "Schroeppel",
                "features",
                "level-based",
                "effectively",
                "transform",
                "enumeration"
            ]
        }
    },
    {
        "id": "49f95db9-bb8c-4ac8-87e2-f57db4ee766a",
        "title": "",
        "chunk_text": "6 Subset Sum Solver with Double Meet-in-the-Middle Optimization Having described our foundational Unique Subset Sums enumerator—which generates k-subsets (for k ≤n 2 per split) and produces only the distinct subset sums (of effective size U = |Σ(S)|)—we now integrate it into a complete Subset Sum solver. We begin by splitting the input set S into two lists, ℓ0 and ℓ1.",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Sum",
                "Sums",
                "Solver",
                "Double",
                "Optimization",
                "Unique",
                "enumerator",
                "k-subsets",
                "split"
            ]
        }
    },
    {
        "id": "4192a1e8-6ff4-466c-8250-4b4b3e322e15",
        "title": "",
        "chunk_text": "For each split, we run our enumerator to generate all unique subset sums, reducing the effective search space from 2n/2 potential sums in each half down to U0 and U1 distinct sums (with U0, U1 ≤U). In many practical instances, U is substantially smaller than the worst-case bound. Crucially, we do not need to wait until each enumerator completes in order to check for a viable solution.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "split",
                "reducing",
                "potential",
                "distinct",
                "run",
                "generate",
                "unique",
                "subset",
                "effective"
            ]
        }
    },
    {
        "id": "cffb814c-5996-4d84-b3ee-78dd3a39c566",
        "title": "",
        "chunk_text": "Instead, as soon as a new unique subset sum is produced in one split, we combine it in real time with sums (and complement logic) from the other split, using a double meet-in-the-middle strategy. On average, verifying each new partial sum requires requires a constant number of lookups to see whether it forms a solution with a counterpart from the other split—whether by checking a direct complement or merging two partial sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "split",
                "strategy",
                "logic",
                "double",
                "complement",
                "sum",
                "sums",
                "unique",
                "subset",
                "produced"
            ]
        }
    },
    {
        "id": "40de9e76-233a-4a63-84c2-0ae24424d93f",
        "title": "",
        "chunk_text": "This on-the-fly procedure is supported by the following lemmas: Lemma (1): Let A ⊂ℓx and B ⊂ℓy. Then sum(A ∪B) = t ⇐⇒ t −sum(A) = sum(B). 15 Lemma (2): For each split independently, a unique subset sum corresponding to the complement of a subset (denoted Ac) yields a valid solution if sum(Ac) = sum(ℓ) −t. Lemma (3): For two splits, if the complements of subsets from both splits (denoted Ac and Bc) satisfy sum(Ac ∪Bc) = sum(S) −t, then a valid solution is found.",
        "metadata": {
            "author": "",
            "keywords": [
                "sum",
                "Lemma",
                "procedure",
                "denoted",
                "supported",
                "valid",
                "solution",
                "lemmas",
                "subset",
                "splits"
            ]
        }
    },
    {
        "id": "59e81727-eefd-4d17-8c99-ff7d61929655",
        "title": "",
        "chunk_text": "Lemma (4): Similarly, if a subset from one split and the complement of a subset from the other (denoted Bc) satisfy sum(A ∪Bc) = t, then a valid solution is obtained. These lemmas cover all ways to combine sums from the two splits. By working with the unique subset sums (of effective size U) rather than all 2n/2 candidate sums, the number of candidate combinations is dramatically reduced in structured instances, directly reflecting the efficiency gained by our enumerator.",
        "metadata": {
            "author": "",
            "keywords": [
                "Similarly",
                "subset",
                "sums",
                "denoted",
                "satisfy",
                "obtained",
                "complement",
                "valid",
                "solution",
                "candidate"
            ]
        }
    },
    {
        "id": "aad5e00d-a020-494e-b48d-f422abf9395a",
        "title": "",
        "chunk_text": "By merging complementary sums from each half, our method verifies candidate solutions in constant time on average, thereby avoiding the costly sort/merge/list-scan step of the classical approach. For the complete pseudocode of our double meet-in-the-middle algorithm, please refer to Appendix A. 6.1 Time Complexity Analysis Splitting the input S into two lists ℓ0 and ℓ1 takes O(n) time.",
        "metadata": {
            "author": "",
            "keywords": [
                "merge",
                "time",
                "half",
                "average",
                "sort",
                "list-scan",
                "approach",
                "merging",
                "complementary",
                "sums"
            ]
        }
    },
    {
        "id": "cdee5a6a-051f-4fe6-9374-dddf06135757",
        "title": "",
        "chunk_text": "For each split, we enumerate all unique subset sums using our modified enumerator (which generates k-subsets for k ≤n 2 ).",
        "metadata": {
            "author": "",
            "keywords": [
                "split",
                "enumerator",
                "enumerate",
                "unique",
                "subset",
                "sums",
                "modified",
                "generates",
                "k-subsets"
            ]
        }
    },
    {
        "id": "4cc734c9-6c56-47b4-a4de-ade658c373ab",
        "title": "",
        "chunk_text": "In the worst case, the total number of k-subsets generated per split is bounded by n/2 X k=0 \u0012n/2 k \u0013 ≤2 \u0012n/2 n/4 \u0013 Since both splits are processed independently and the solution verification (using Lemma (1) and related checks) incurs only a logarithmic cost per k-subset, the overall time complexity of the solver is e O \u0012\u0012n/2 n/4 \u0013\u0013 ≈e O \u00122n/2 √n \u0013 ≈e O \u0010 2n/2\u0011 which in structured instances effectively becomes e O(U), where U ≪2n/2.",
        "metadata": {
            "author": "",
            "keywords": [
                "Lemma",
                "case",
                "verification",
                "checks",
                "incurs",
                "worst",
                "total",
                "number",
                "generated",
                "bounded"
            ]
        }
    },
    {
        "id": "1675b2a7-473b-46b6-b5b3-afba0b5e0b7d",
        "title": "",
        "chunk_text": "Thus, by parameterizing the algorithm in terms of U (the effective number of unique subset sums), we obtain a refined understanding of instance hardness and a significant practical improvement over classical methods. For the complete pseudocode of our double meet-in-the-middle algorithm, please refer to Appendix A.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "methods",
                "algorithm",
                "parameterizing",
                "terms",
                "effective",
                "number",
                "unique",
                "subset",
                "obtain"
            ]
        }
    },
    {
        "id": "4e0a31ce-81b9-449b-9716-f30719d9bcb7",
        "title": "",
        "chunk_text": "16 7 Meet-in-the-Middle Speed-Up via Combinatorial Tree Compression Meet-in-the-middle (MIM) is a classical approach to Subset Sum (and related problems) that splits the input set S of size n into two lists of size ≈n/2 each. It then enumerates all 2n/2 possible subsets in each half, producing two lists of partial sums, and merges them in time e O(2n/2) [1].",
        "metadata": {
            "author": "",
            "keywords": [
                "MIM",
                "Compression",
                "Combinatorial",
                "Tree",
                "size",
                "lists",
                "Speed-Up",
                "problems",
                "Sum",
                "sums"
            ]
        }
    },
    {
        "id": "fbcbbb4f-d5b7-4e59-b0a2-e9ded5b3017c",
        "title": "",
        "chunk_text": "Below, we describe a simple yet effective technique that reduces the exponent by a noticeable constant factor, while preserving correctness. Crucially, we emphasize that this method does not actually modify the problem instance. Instead, it reshapes the combinatorial enumeration tree so that a significant portion of the naive meet-in-the-middle expansions are collapsed (i.e. pruned) earlier. Classical MIM Recap. Recall that in the standard MIM algorithm: 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "factor",
                "correctness",
                "describe",
                "simple",
                "effective",
                "technique",
                "reduces",
                "exponent",
                "noticeable",
                "constant"
            ]
        }
    },
    {
        "id": "c4b8a5eb-1320-4874-a829-40607fb1cf18",
        "title": "",
        "chunk_text": "We split S into two disjoint subsets ℓ0 and ℓ1, each of size n/2. 2. For each half, we enumerate all 2n/2 subsets and record their sums. 3. One list is sorted or hashed; we then look for complementary sums from the other list to see if the target t can be matched. This yields a worst-case time bound on the order of e O(2n/2) and a space usage of O(2n/2). Combinatorial Tree Compression (“Injected Collisions”). We can shrink the exponent by about 0.415 by “forcing” a duplicate inside each half.",
        "metadata": {
            "author": "",
            "keywords": [
                "subsets",
                "half",
                "sums",
                "split",
                "disjoint",
                "size",
                "enumerate",
                "record",
                "list",
                "Compression"
            ]
        }
    },
    {
        "id": "f5eb1fab-d930-463f-9e75-5382bc672c4c",
        "title": "",
        "chunk_text": "A common way to describe this is: in each half ℓi, replace the smallest element by the second-smallest element. However, a clearer interpretation is that we do not truly alter the set S; rather, we inject a short-circuit into the enumeration logic so that many subset sums from ℓi collide earlier. Hence, large portions of the search tree are recognized as duplicates and pruned. Formally, let x0, x1 be the two smallest elements in half ℓ0 (so x0 ≤x1).",
        "metadata": {
            "author": "",
            "keywords": [
                "replace",
                "common",
                "describe",
                "second-smallest",
                "smallest",
                "element",
                "half",
                "earlier",
                "elements",
                "clearer"
            ]
        }
    },
    {
        "id": "9e9a8043-2eb4-470d-ac5e-876fd2813123",
        "title": "",
        "chunk_text": "We then tag x0 as a “duplicate-forcing element” that always merges into the branch for x1 if a collision arises. An analogous choice is made in the other half ℓ1. When carrying out MIM, each partial sum also keeps track of a “duplicate-usage flag.” If two partial sums differ only in which copy of x1 they used, they are merged into a single node. Resulting Exponential Reduction. Consider two identical elements, say x0 and x1, in one half of the split.",
        "metadata": {
            "author": "",
            "keywords": [
                "tag",
                "duplicate-forcing",
                "arises",
                "merges",
                "branch",
                "collision",
                "partial",
                "MIM",
                "half",
                "Reduction"
            ]
        }
    },
    {
        "id": "1add8e17-142e-41e0-be35-eafb43bda973",
        "title": "",
        "chunk_text": "Normally, when considering whether to include these two elements, there are four possible choices: 1. Include neither: contributes 0. 2. Include only x0: contributes x0. 3. Include only x1: contributes x1. 4. Include both: contributes x0 + x1. If x0 and x1 are treated as distinct, these four options yield four different sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "contributes",
                "include",
                "elements",
                "choices",
                "distinct",
                "sums",
                "treated",
                "options",
                "yield"
            ]
        }
    },
    {
        "id": "0410255e-1647-4332-b02f-6361dbce7cd7",
        "title": "",
        "chunk_text": "However, if we force x0 to behave as a duplicate of x1 — that is, if we merge the cases where only x0 is included with those where only x1 is included — then the two middle cases produce the same sum. As a result, the four possibilities collapse into only three unique outcomes. 17 In effect, for every block of 4 potential combinations, we obtain only 3 unique sums. This is a reduction by a factor of 3 4 (i.e., we retain 75% of the original count).",
        "metadata": {
            "author": "",
            "keywords": [
                "included",
                "cases",
                "force",
                "behave",
                "duplicate",
                "merge",
                "middle",
                "produce",
                "unique",
                "sum"
            ]
        }
    },
    {
        "id": "d0fc934c-bf7b-4922-a01d-563d6eba488d",
        "title": "",
        "chunk_text": "When applied uniformly to an entire half that originally yields 2n/2 sums, the new count becomes 0.75 × 2n/2 We can express the multiplier 0.75 as: 0.75 = 2−log2(4/3) where log2 \u00124 3 \u0013 ≈0.415 Thus, the total number of unique subset sums in that half becomes: 2n/2−0.415 This means that, while the worst-case complexity remains exponential, the effective exponent is reduced by about 0.415 for each half of the split.",
        "metadata": {
            "author": "",
            "keywords": [
                "half",
                "sums",
                "yields",
                "multiplier",
                "exponential",
                "split",
                "applied",
                "uniformly",
                "entire",
                "originally"
            ]
        }
    },
    {
        "id": "2d47edba-b27c-4947-8334-81eed051c627",
        "title": "",
        "chunk_text": "In practical terms, if one half originally produced 2n/2 unique sums, forcing a duplicate lowers that number to approximately 2n/2−0.415, leading to a significant constant-factor speedup in the overall algorithm. Key Clarification: Not Modifying the Instance. Despite the phrase “force a duplicate,” we do not literally change any element in the original set S that we want to solve.",
        "metadata": {
            "author": "",
            "keywords": [
                "terms",
                "produced",
                "unique",
                "sums",
                "forcing",
                "approximately",
                "leading",
                "algorithm",
                "duplicate",
                "practical"
            ]
        }
    },
    {
        "id": "8712336f-3312-4691-9f65-106ac15ad903",
        "title": "",
        "chunk_text": "Instead, we slightly reorganize the enumerator so that subsets including what was x0 always merge with subsets including x1 (and similarly in ℓ1). Hence, it is more accurate to say we inject collisions into the enumerator’s search tree rather than rewriting the input. The target remains the same, and correctness is preserved with a small amount of extra bookkeeping (e.g., a boolean flag to track the “collision merges”).",
        "metadata": {
            "author": "",
            "keywords": [
                "subsets",
                "including",
                "enumerator",
                "slightly",
                "reorganize",
                "similarly",
                "input",
                "merge",
                "merges",
                "collisions"
            ]
        }
    },
    {
        "id": "606e3df4-5631-41e9-b5f3-3a1680d594ce",
        "title": "",
        "chunk_text": "Numerical checks comparing sums from ℓ0 and ℓ1 then require only constant additional effort. Further Enhancements As a practical matter, one injected collision in the format of forced duplicate per split is often enough to cut the unique sums in each half by roughly 25%, thereby reducing the exponent from n 2 to about n 2 −0.415.",
        "metadata": {
            "author": "",
            "keywords": [
                "Numerical",
                "effort",
                "sums",
                "checks",
                "comparing",
                "require",
                "constant",
                "additional",
                "Enhancements",
                "matter"
            ]
        }
    },
    {
        "id": "e2f2d3ec-3c21-4e38-be72-ab405fe6187f",
        "title": "",
        "chunk_text": "If memory is especially constrained, one can try forcing two elements per split, potentially cutting unique sums by an additional factor of 0.75 for each extra duplicate. However, each extra duplicate also increases the meet-step overhead (since you must check more “usage-flag” combinations).",
        "metadata": {
            "author": "",
            "keywords": [
                "extra",
                "constrained",
                "split",
                "potentially",
                "duplicate",
                "usage-flag",
                "combinations",
                "memory",
                "forcing",
                "elements"
            ]
        }
    },
    {
        "id": "cd935459-e215-4ba1-b8ce-a6a35043a76e",
        "title": "",
        "chunk_text": "In principle, the idea can be extended further or combined with other collision-based heuristics to prune even more subsets, but each additional forced duplicate must balance enumeration savings against added overhead. Hence the “one or two forced duplicates per half” approach is usually a sweet spot, though more sophisticated or specialized schemas can offer further memory and time savings if carefully managed. Half-Subset Enumeration Plus Injected Collisions.",
        "metadata": {
            "author": "",
            "keywords": [
                "principle",
                "subsets",
                "overhead",
                "forced",
                "enumeration",
                "savings",
                "idea",
                "extended",
                "combined",
                "collision-based"
            ]
        }
    },
    {
        "id": "02b24f8a-19c9-416f-a1a1-a493407e1987",
        "title": "",
        "chunk_text": "By default, our enumerator lists only those subsets of size at most n/4 in each half (relying on complements for larger subsets). This alone cuts naive meet-in-the-middle enumeration by a factor of 2, leaving 2 n/2−1 expansions per half instead of 2n/2. 18 Injection a collision by forcing one duplicate in each split (that is, exactly one forced duplicate per half) reduces the overall enumeration cost by an additional 25% factor, i.e. multiply by 0.75.",
        "metadata": {
            "author": "",
            "keywords": [
                "subsets",
                "half",
                "default",
                "relying",
                "enumerator",
                "lists",
                "size",
                "complements",
                "larger",
                "Injection"
            ]
        }
    },
    {
        "id": "b3676280-1dff-48f5-ab1c-e60840b8a6a4",
        "title": "",
        "chunk_text": "Hence, combining with half-subset enumeration yields 0.5 × 0.75 = 0.375 so we end up enumerating only about 37.5% of the naive 2n/2 expansions in practice. If we force two duplicates in each split, we observe another 25% compound reduction ( 0.752 = 0.5625), giving 0.5 × 0.752 = 0.28125 or about 28% of the naive expansions.",
        "metadata": {
            "author": "",
            "keywords": [
                "combining",
                "yields",
                "practice",
                "naive",
                "expansions",
                "half-subset",
                "enumeration",
                "end",
                "enumerating",
                "giving"
            ]
        }
    },
    {
        "id": "ead70e5f-7482-4838-b276-25f7a30c4051",
        "title": "",
        "chunk_text": "Universality and Guaranteed Pruning A final point is that this combinatorial tree compression via the forced-duplicate method applies uniformly to all input instances, whether highly structured or not; it integrates seamlessly into our enumerator with no special-case logic or side effects. In essence, once duplicates are forced, every instance behaves as though it had some intrinsic collision structure, because large parts of the combinatorial tree merge early.",
        "metadata": {
            "author": "",
            "keywords": [
                "Guaranteed",
                "Pruning",
                "Universality",
                "effects",
                "combinatorial",
                "tree",
                "final",
                "point",
                "compression",
                "forced-duplicate"
            ]
        }
    },
    {
        "id": "cd04e5a6-674c-4e71-a67a-df27b5faf29d",
        "title": "",
        "chunk_text": "8 Divide and Conquer for Anytime Running Time In this section, we present a divide-and-conquer variant of our baseline solver without the combinatorial tree compression optimization. This choice lets us illustrate the incremental and “resume” behavior more transparently. Our goal is to show how the algorithm can progressively explore the problem space—whose effective size is determined by U = |Σ(S)| (the number of unique subset sums)—in an anytime fashion.",
        "metadata": {
            "author": "",
            "keywords": [
                "Divide",
                "Conquer",
                "Running",
                "Time",
                "section",
                "variant",
                "optimization",
                "Anytime",
                "present",
                "baseline"
            ]
        }
    },
    {
        "id": "d227edd1-57a9-489f-8281-a70c279a2a63",
        "title": "",
        "chunk_text": "This variant also highlights how we leverage the seeding and branch enumeration features of our enumerator. 8.1 Overview of the Optimization Recall that our basic enumeration process expands k-permutations column by column to generate all feasible unique subset sums for each split, thereby ultimately producing a set of distinct sums of size U, which is typically much smaller than the full 2n possibilities.",
        "metadata": {
            "author": "",
            "keywords": [
                "enumerator",
                "enumeration",
                "variant",
                "highlights",
                "leverage",
                "seeding",
                "branch",
                "features",
                "Overview",
                "column"
            ]
        }
    },
    {
        "id": "7e1a1f9c-0a77-4cfa-a6cf-abdc2666e2ea",
        "title": "",
        "chunk_text": "In addition to this standard expansion, our enumerator supports seeding: if the initial INPUT contains partially developed k-permutations, their accumulated sums (which contribute to the overall U) are added to the memoization structure, and the enumeration resumes from these prefixes. This seeding capability ensures that intermediate results are retained and re-utilized, thereby connecting successive enumeration cycles while avoiding redundant work over the same unique sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "INPUT",
                "sums",
                "expansion",
                "k-permutations",
                "structure",
                "prefixes",
                "seeding",
                "enumeration",
                "addition",
                "standard"
            ]
        }
    },
    {
        "id": "3c2b3c1b-23de-4e39-a114-260ca746a687",
        "title": "",
        "chunk_text": "Figure 1 shows the number of k-subsets generated in each column expansion sub-process for an input instance with n = 48. In this case, the process is executed n/4 = 12 times, yielding a total of 8 388 607 k-subsets per split, which corresponds to the expected 2n/2 2 in the worst case. The memoization structure holds only half of the distinct subset sums, reflecting the true effective search space. 19 Figure 1 Counts of k-subsets enumerated per column expansion (input n = 48).",
        "metadata": {
            "author": "",
            "keywords": [
                "Figure",
                "k-subsets",
                "shows",
                "case",
                "number",
                "generated",
                "sub-process",
                "instance",
                "column",
                "expansion"
            ]
        }
    },
    {
        "id": "593eae43-8d0e-4e02-831f-5baa80ed3fee",
        "title": "",
        "chunk_text": "Our improved approach modifies this behavior by slicing the enumeration into multi- ple cycles rather than performing one long cycle. Each cycle handles a different slice of the problem—updating and extending the memoized unique subset sums (up to size U) incrementally—as illustrated in Figure 2 for the same n = 48 instance. Figure 2 Run-time graphs for n = 48 with optimization enabled.",
        "metadata": {
            "author": "",
            "keywords": [
                "Figure",
                "multi",
                "ple",
                "cycle",
                "improved",
                "approach",
                "modifies",
                "behavior",
                "slicing",
                "enumeration"
            ]
        }
    },
    {
        "id": "8946b51d-8cc3-40d1-83e6-07ca4d827444",
        "title": "",
        "chunk_text": "The left panel shows the most computationally expensive column expansion in each cycle, while the right panel illustrates the number of subsets enumerated in each cycle. 8.2 The Look-Ahead and Rescheduling Mechanism Our optimization uses a look-ahead strategy to decide whether to postpone (reschedule) the expansion of a k-permutation to a later cycle, this strategy is customizable, the chosen mechanism for our experiments works as follows: 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "cycle",
                "panel",
                "expansion",
                "Mechanism",
                "left",
                "shows",
                "computationally",
                "expensive",
                "column",
                "illustrates"
            ]
        }
    },
    {
        "id": "2ed8f788-269c-4210-ba36-428f2804bcd3",
        "title": "",
        "chunk_text": "For each k-permutation about to expand a new column, we examine a block of candidate elements. The size of this block is determined by look-ahead =          0, if 2n > SUM(S), \u0004 n 16 \u0005 +    n 32 + 1, if n 32 > 1, 0, otherwise, otherwise. In very dense instances—i.e., when the total number of possible subset combinations 2n exceeds the available numerical space SUM(S) (the sum of all elements in S)—the search space is saturated and the look-ahead is set to 0.",
        "metadata": {
            "author": "",
            "keywords": [
                "SUM",
                "column",
                "block",
                "k-permutation",
                "expand",
                "examine",
                "candidate",
                "elements",
                "look-ahead",
                "space"
            ]
        }
    },
    {
        "id": "1ea57425-4786-48a2-bf7c-ae14106c55d7",
        "title": "",
        "chunk_text": "Otherwise, the look-ahead scales with n as shown above. 20 2. During the i-th column expansion of the current cycle, we consider a block of candidate elements spanning from the i-th position up to the (i + look-ahead)-th position. For each candidate element in this block that is not already included in the current k-permutation, we compute the tentative new subset sum obtained by including that element. 3.",
        "metadata": {
            "author": "",
            "keywords": [
                "scales",
                "shown",
                "look-ahead",
                "current",
                "candidate",
                "i-th",
                "element",
                "block",
                "position",
                "elements"
            ]
        }
    },
    {
        "id": "bf737966-6851-44c8-9f25-a2831a246934",
        "title": "",
        "chunk_text": "If this tentative sum has already been recorded in our memoization table (i.e., it is not unique among the U values), we do not expand the current k-permutation in the current cycle. Instead, we determine a future cycle for expansion based on the candidate element’s position within the look-ahead block, futureCycle = currentCycle + (position of the candidate element in the block) and then reschedule the k-permutation to be expanded in that future cycle.",
        "metadata": {
            "author": "",
            "keywords": [
                "current",
                "cycle",
                "k-permutation",
                "table",
                "candidate",
                "block",
                "tentative",
                "sum",
                "recorded",
                "memoization"
            ]
        }
    },
    {
        "id": "a4e8ec26-2ff4-4b8f-a11b-818c591acb8e",
        "title": "",
        "chunk_text": "This look-ahead and rescheduling mechanism forces the algorithm to prioritize the expansion of those k-permutations that are most likely to yield new unique subset sums (i.e., contributing to U) early in the process. In contrast, those that are rescheduled—due to diminished potential from redundancy—produce fewer new sums when eventually expanded.",
        "metadata": {
            "author": "",
            "keywords": [
                "contributing",
                "early",
                "process",
                "sums",
                "look-ahead",
                "rescheduling",
                "mechanism",
                "forces",
                "algorithm",
                "prioritize"
            ]
        }
    },
    {
        "id": "d812aa57-5194-47b6-961f-3f88d3feb82c",
        "title": "",
        "chunk_text": "Note that the seeding mechanism ensures that any previously computed unique subset sums are retained and reintroduced in subsequent cycles, thus maintaining continuity. The chosen look-ahead formula and rescheduling strategy ensure that each cycle processes roughly half of the deferred work from the previous cycle, resulting in a geometric decay in the workload.",
        "metadata": {
            "author": "",
            "keywords": [
                "Note",
                "continuity",
                "cycle",
                "seeding",
                "mechanism",
                "previously",
                "computed",
                "unique",
                "subset",
                "sums"
            ]
        }
    },
    {
        "id": "6e2b7708-8d7a-4f39-93ed-bd18a15f475a",
        "title": "",
        "chunk_text": "In practice, the look-ahead mechanism is exhausted for each k-permutation as it nears its maximum expansion (approximately n/4 elements), at which point no further rescheduling is required and all pending k-permutations complete their expansion.",
        "metadata": {
            "author": "",
            "keywords": [
                "elements",
                "expansion",
                "practice",
                "approximately",
                "look-ahead",
                "mechanism",
                "exhausted",
                "maximum",
                "point",
                "rescheduling"
            ]
        }
    },
    {
        "id": "be4b3469-c988-49d5-a44d-ce7ec6db0f33",
        "title": "",
        "chunk_text": "Formally, if we denote by W0 the work (i.e., the number of unique subset sums) performed in the first cycle, then the work in cycle i is approximately Wi ≈W0 2i Thus, the total work across all cycles is the sum of a geometric series: Wtotal = ∞ X i=0 Wi ≈ ∞ X i=0 W0 2i = 2W0 Although the series terminates after a finite number of cycles, this analysis shows that the dominant contribution comes from the early cycles and that the work halves from one cycle to the next.",
        "metadata": {
            "author": "",
            "keywords": [
                "work",
                "cycle",
                "cycles",
                "Wtotal",
                "number",
                "series",
                "Formally",
                "performed",
                "denote",
                "unique"
            ]
        }
    },
    {
        "id": "26483489-f930-4d56-9f6e-b10ef0f86f6d",
        "title": "",
        "chunk_text": "8.3 A Brief Example for Anytime/Online Usage To illustrate the incremental power of this approach, suppose you run the solver for two cycles on a split of 16 elements. You can pause the solver and observe the partial unique subset sums (i.e., a portion of Σ(S) with effective size U), or even insert a new element xnew into the same split.",
        "metadata": {
            "author": "",
            "keywords": [
                "Anytime",
                "Online",
                "Usage",
                "solver",
                "approach",
                "suppose",
                "split",
                "illustrate",
                "incremental",
                "power"
            ]
        }
    },
    {
        "id": "46a860d8-3d8d-4314-a407-33e35958bf76",
        "title": "",
        "chunk_text": "Since the enumerator retains all memoized data (unique sums and its corresponding k-subsets), the arrival of xnew spawns new branches from the relevant prefixes without requiring a full rebuild. This process exemplifies the anytime property: at each cycle boundary, you have a partial solution that may already solve simpler targets or indicate that no solution exists under certain conditions.",
        "metadata": {
            "author": "",
            "keywords": [
                "data",
                "unique",
                "k-subsets",
                "rebuild",
                "enumerator",
                "retains",
                "memoized",
                "sums",
                "arrival",
                "xnew"
            ]
        }
    },
    {
        "id": "62205acf-04d1-4b16-ad6c-c2479adf55f1",
        "title": "",
        "chunk_text": "21 8.4 Experiments and Time Analysis for the Anytime Runtime We conducted a series of experiments on worst-case input instances to demonstrate the anytime behavior of the algorithm. These experiments were run on a system with 64GB RAM and a 12th Gen i7-12700 processor. Figure 3 illustrates how the Unique Subset Sums generation process evolves for input instances of lengths 32, 40, 48, and 56.",
        "metadata": {
            "author": "",
            "keywords": [
                "Anytime",
                "Time",
                "Analysis",
                "Runtime",
                "Experiments",
                "algorithm",
                "Gen",
                "input",
                "instances",
                "conducted"
            ]
        }
    },
    {
        "id": "5adf5b85-3bba-4f62-919e-0a62bb0c30f6",
        "title": "",
        "chunk_text": "In each graph, the left panel displays the most computationally expensive column expansion in each cycle, while the right panel illustrates the number of subsets enumerated in each cycle. In all cases, the first enumeration cycle generates the most k-subsets, which confirms our expectation from the prioritization logic described earlier—following a geometric series. Figure 3 Anytime run-time graphs for input instances of lengths 32, 40, 48, and 56.",
        "metadata": {
            "author": "",
            "keywords": [
                "panel",
                "cycle",
                "left",
                "displays",
                "computationally",
                "expensive",
                "column",
                "expansion",
                "illustrates",
                "number"
            ]
        }
    },
    {
        "id": "611c30f3-d82d-47fe-a22a-95d3225cc207",
        "title": "",
        "chunk_text": "8.5 Intuition Behind the Slicing Mechanism A key observation is that the structure of the generated k-permutations exhibits a fractal- like behavior. When visualized by the ordinal positions of the elements, the structure remains consistent as the input size increases; only additional layers are added as the length of the input increases with additional elements. This structural property allows our look-ahead approach to scale efficiently when handled at the proper granularity level.",
        "metadata": {
            "author": "",
            "keywords": [
                "Intuition",
                "Slicing",
                "Mechanism",
                "structure",
                "fractal",
                "behavior",
                "elements",
                "key",
                "observation",
                "generated"
            ]
        }
    },
    {
        "id": "2b43374b-8b9f-4a6c-8625-ee025c5dc992",
        "title": "",
        "chunk_text": "22 8.6 Initial Solution Time Complexity and Improvement Over Time Estimation The worst-case initial solution time complexity for input instances with 2n/2 Unique Subset Sums per split—when the chosen look-ahead and rescheduling mechanism is applied—is given by: O \u0012 poly(n) · \u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0013 ≈e O \u0012\u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0013 and the work per cycle decreases progressively according to: Wi ≈W0 2i where i is the iteration index and the maximum number of expansion cycles is n/4 9 Additive-Combinatorial Underpinnings In this section, we elaborate on how classical results from additive combinatorics underpin the “effective search space” parameter U = |Σ(S)|.",
        "metadata": {
            "author": "",
            "keywords": [
                "Time",
                "Solution",
                "Initial",
                "Complexity",
                "Unique",
                "Improvement",
                "Estimation",
                "Subset",
                "Sums",
                "Underpinnings"
            ]
        }
    },
    {
        "id": "2dbfc273-b305-403b-983c-cde50e410aa7",
        "title": "",
        "chunk_text": "In particular, phenomena such as small doubling, high additive energy, duplicate elements, and specialized sequences explain why many real or “structured” instances of Subset Sum exhibit U ≪2n, thereby enabling substantial pruning in our algorithm. 9.1 Small Doubling and Freiman-Type Embeddings A set A ⊂Z (or R) is said to have small doubling if |A + A| ≤C |A| for some constant C. Equivalently, A + A is not much larger than A itself.",
        "metadata": {
            "author": "",
            "keywords": [
                "small",
                "doubling",
                "Subset",
                "Sum",
                "structured",
                "phenomena",
                "high",
                "energy",
                "duplicate",
                "elements"
            ]
        }
    },
    {
        "id": "441ffefd-89e9-4dae-988b-c17535b176e1",
        "title": "",
        "chunk_text": "A fundamental result by Freiman [14] (see also Tao [17] and Tao–Vu [18]) shows that such sets can be embedded into a low-dimensional generalized arithmetic progression (GAP). Roughly stated: ▶Theorem 8 (Freiman’s Theorem, informal). If A ⊂Z satisfies |A + A| ≤C |A|, then there exists a low-dimensional generalized arithmetic progression P such that A ⊆P and |P| ≤α |A| for some constant α. The dimension depends on C.",
        "metadata": {
            "author": "",
            "keywords": [
                "Tao",
                "GAP",
                "Freiman",
                "Theorem",
                "low-dimensional",
                "generalized",
                "arithmetic",
                "shows",
                "progression",
                "fundamental"
            ]
        }
    },
    {
        "id": "3bfc07de-5e31-4eb5-8f6a-6ab2c78b0143",
        "title": "",
        "chunk_text": "Such low-dimensional progressions do not “explode” combinatorially under addition—many partial sums coincide. Consequently, if a set S has small doubling, the total number of distinct subset sums |Σ(S)| can be far below the worst-case 2n bound.",
        "metadata": {
            "author": "",
            "keywords": [
                "explode",
                "combinatorially",
                "addition",
                "coincide",
                "bound",
                "sums",
                "low-dimensional",
                "progressions",
                "partial",
                "doubling"
            ]
        }
    },
    {
        "id": "a7694574-73b1-4e97-942d-de86f8ebd26d",
        "title": "",
        "chunk_text": "In fact, by applying Freiman’s theorem more directly, any set S ⊂Z of size n with |S + S| ≤K|S| is contained in a generalized arithmetic progression (GAP) of dimension OK(1) and size at most f(K) n; hence, |Σ(S)| ≤nOK(1), which is exponentially smaller than 2n. In our solver (see Section 5), each collision in the combinatorial tree prunes branches that would otherwise enumerate duplicate sums, reducing the naive 2n/2 partial sums to an effective count on the order of |Σ(S)|.",
        "metadata": {
            "author": "",
            "keywords": [
                "GAP",
                "size",
                "Freiman",
                "nOK",
                "fact",
                "directly",
                "progression",
                "applying",
                "theorem",
                "set"
            ]
        }
    },
    {
        "id": "90322a4d-c369-49a8-8752-32064eebfafb",
        "title": "",
        "chunk_text": "9.2 Additive Energy and Collision-Based Pruning An alternative measure of additive structure is the additive energy E(A) of a finite set A, defined by E(A) = {(a, b, c, d) ∈A4 : a + b = c + d} 23 A high additive energy indicates that many pairs (a, b) and (c, d) yield the same sum a + b = c + d.",
        "metadata": {
            "author": "",
            "keywords": [
                "Additive",
                "Energy",
                "Pruning",
                "defined",
                "pairs",
                "yield",
                "Collision-Based",
                "alternative",
                "measure",
                "structure"
            ]
        }
    },
    {
        "id": "65b364d2-6e21-4e98-8148-0fcacf58ec1d",
        "title": "",
        "chunk_text": "This strong overlap again suggests fewer unique sums, so the set A is “collision- rich.” Results like the Balog–Szemerédi–Gowers theorem ([18]) imply that if E(A) ≥|A|3 K , then a large subset of A can be embedded in a small-doubling configuration. Even when a set does not exhibit small doubling prima facie, high additive energy thus forces many collisions in partial sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "rich",
                "sums",
                "strong",
                "overlap",
                "suggests",
                "fewer",
                "unique",
                "Szemerédi",
                "set",
                "Balog"
            ]
        }
    },
    {
        "id": "02b1dbf2-9619-4a80-9c63-4e2fa2dfbaa8",
        "title": "",
        "chunk_text": "In our context, whenever the enumerator attempts to extend a partial sum that has already been generated, it detects a duplicate and prunes that branch. Hence, large additive energy—and thus frequent collisions—dramatically reduces the effective number of expansions needed. Our enumerator never re-lists an already-encountered sum. High energy =⇒more collisions =⇒far fewer newly generated sums. For more on bounding E(S) and its structural implications, we refer to [19, 20] and references therein.",
        "metadata": {
            "author": "",
            "keywords": [
                "context",
                "branch",
                "energy",
                "collisions",
                "attempts",
                "extend",
                "partial",
                "detects",
                "duplicate",
                "prunes"
            ]
        }
    },
    {
        "id": "757cc583-3c91-454f-83bf-bc4d7defabcd",
        "title": "",
        "chunk_text": "9.3 Duplicates and Specialized Sequences Beyond small doubling and high additive energy, other specific properties further diminish |Σ(S)|. Two prominent examples are duplicate elements and specialized sequences that exhibit near-linear or progression-like behavior. 9.3.0.1 Duplicates. When S contains duplicates, many different subsets produce the same sum because repeated values contribute identically.",
        "metadata": {
            "author": "",
            "keywords": [
                "Specialized",
                "Sequences",
                "Duplicates",
                "energy",
                "diminish",
                "small",
                "doubling",
                "high",
                "additive",
                "specific"
            ]
        }
    },
    {
        "id": "6d788bc9-2868-4516-9d52-7187cc52eb6c",
        "title": "",
        "chunk_text": "This redundancy effectively prunes the search space, reducing the effective number of unique sums. 9.3.0.2 Specialized Sequences. Certain highly structured sequences—such as arithmetic progressions, quadratic sequences, or Fibonacci-like sequences—tend to produce overlapping subset sums, which significantly reduce the effective search space U. For example, a proper arithmetic progression P of length L satisfies (see [18]) |Σ(P)| ≤O(L2) a dramatic reduction from the worst-case 2L.",
        "metadata": {
            "author": "",
            "keywords": [
                "Sequences",
                "reducing",
                "sums",
                "search",
                "space",
                "effective",
                "Specialized",
                "redundancy",
                "effectively",
                "prunes"
            ]
        }
    },
    {
        "id": "7fb9463b-b5b4-4c4a-9c66-448505b17462",
        "title": "",
        "chunk_text": "In contrast, geometric progressions (with a common ratio of at least 2) typically yield distinct subset sums due to their exponential growth. Ultimately, it is these near-linear or progression-like structures that yield heavy sum collisions, decreasing U and reducing enumeration cost.",
        "metadata": {
            "author": "",
            "keywords": [
                "contrast",
                "geometric",
                "progressions",
                "typically",
                "growth",
                "Ultimately",
                "yield",
                "common",
                "ratio",
                "distinct"
            ]
        }
    },
    {
        "id": "7c2fcdcb-8675-482a-aecf-99cea9fd55a2",
        "title": "",
        "chunk_text": "9.4 Container Methods Recent advances in additive combinatorics, particularly hypergraph container methods [21, 22], show that sets with small doubling or high additive energy can be “contained” in families of structured sets (e.g., small GAPs). These techniques yield tight bounds on how many sets of a given size possess such properties, reinforcing the idea that collision-rich instances are relatively common in certain distributions.",
        "metadata": {
            "author": "",
            "keywords": [
                "Container",
                "Methods",
                "contained",
                "Recent",
                "additive",
                "small",
                "combinatorics",
                "show",
                "GAPs",
                "sets"
            ]
        }
    },
    {
        "id": "61d9f965-ef5d-48c3-8194-8e261bf7d2b7",
        "title": "",
        "chunk_text": "24 Our algorithm’s dynamic pruning is reminiscent of container-based reasoning: by continu- ally merging branches that yield identical partial sums, we restrict the search to roughly e O(U) expansions rather than enumerating 2n/2 possibilities. Container-based insights thereby help explain why such structure-driven collisions systematically reduce the runtime in practice.",
        "metadata": {
            "author": "",
            "keywords": [
                "possibilities",
                "reasoning",
                "continu",
                "ally",
                "sums",
                "expansions",
                "enumerating",
                "container-based",
                "algorithm",
                "dynamic"
            ]
        }
    },
    {
        "id": "65ff9c48-8da7-4ece-9a1a-c64294d25f9a",
        "title": "",
        "chunk_text": "9.5 A Unified Perspective on Additive Structure Classical measures like small doubling or high additive energy capture global overlap in sums. Specialized sequences and duplicates drive collisions through more explicit local or combinatorial patterns. We introduce additional parameters, such as the linearity factor λ, clustering factor γ, and duplicate measure δ, primarily to track how redundant the subset sums become: Linearity Factor λ: Gauges how near-linear or AP-like the elements in S are.",
        "metadata": {
            "author": "",
            "keywords": [
                "Additive",
                "Unified",
                "Perspective",
                "Structure",
                "Classical",
                "factor",
                "linearity",
                "small",
                "doubling",
                "high"
            ]
        }
    },
    {
        "id": "eb35733a-cfba-4223-beee-60eeb2eb5c89",
        "title": "",
        "chunk_text": "Clustering Factor γ: Indicates how tightly |Σ(S)| clusters in numerical value (many sums coincide or lie close together so subsequent sum expansions collide). Duplicate Measure δ: Quantifies the effective number of repeated elements, leading to merged branches in enumeration. A low doubling constant or high additive energy is effectively another manifestation of these redundancies—they all cause collisions among the partial sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "Factor",
                "Clustering",
                "tightly",
                "clusters",
                "collide",
                "Quantifies",
                "numerical",
                "coincide",
                "lie",
                "close"
            ]
        }
    },
    {
        "id": "e405af05-e006-43fb-a5f7-dfdea3b05c24",
        "title": "",
        "chunk_text": "Crucially, linearity alone does not guarantee collisions (some arithmetic progressions have large step sizes and yield distinct sums), but dense linear structures do force many overlaps. Consequently, these parameters must be viewed as markers of additive redundancy, dictating how quickly or frequently the enumerator merges branches that share a sum.",
        "metadata": {
            "author": "",
            "keywords": [
                "Crucially",
                "linearity",
                "collisions",
                "overlaps",
                "guarantee",
                "arithmetic",
                "progressions",
                "large",
                "step",
                "sizes"
            ]
        }
    },
    {
        "id": "51c2e225-1bc3-47e2-8b2f-e2af6122ef93",
        "title": "",
        "chunk_text": "9.6 Implications for Our Algorithm Whenever S exhibits one or more of these additive structures (small doubling, high energy, large duplicates, near-linear patterns, etc.), it naturally produces overlapping sums, driving down the effective count of unique partial sums U = |Σ(S)|. Our solver’s collision-based pruning directly leverages this overlap: 1. Fewer Explicit Expansions. Each newly formed partial sum is checked against a memo of sums already encountered, pruning branches on any collision.",
        "metadata": {
            "author": "",
            "keywords": [
                "Implications",
                "Algorithm",
                "etc.",
                "structures",
                "small",
                "doubling",
                "high",
                "energy",
                "large",
                "duplicates"
            ]
        }
    },
    {
        "id": "a0e8b3e7-ddc6-46ae-b40a-7ca7322a9004",
        "title": "",
        "chunk_text": "Consequently, the algorithm invests only e O(U) expansions, often far below 2n/2 in structured regimes. 2. Adaptive to Structure. While “worst-case” dissociative sets see no collisions (leading to the classical 2n/2 behavior), typical or real-world instances often exhibit small doubling, high energy, or near-linearity. The algorithm adapts automatically to those structural properties, pruning aggressively and reducing runtime.",
        "metadata": {
            "author": "",
            "keywords": [
                "expansions",
                "regimes",
                "Adaptive",
                "Structure",
                "invests",
                "structured",
                "algorithm",
                "worst-case",
                "behavior",
                "dissociative"
            ]
        }
    },
    {
        "id": "430bff91-f315-4eb8-90cc-b8e0f3343801",
        "title": "",
        "chunk_text": "Hence, from a theoretical perspective, these additive-combinatorics results justify why many Subset Sum instances do not suffer exponential blow-up: they reside in a “collision-rich” region of the input space. From a practical standpoint, detecting such collisions on the fly (rather than enumerating all subsets and only then filtering duplicates) is precisely what our unique-subset-sums enumerator does.",
        "metadata": {
            "author": "",
            "keywords": [
                "collision-rich",
                "Sum",
                "perspective",
                "blow-up",
                "region",
                "space",
                "theoretical",
                "additive-combinatorics",
                "results",
                "justify"
            ]
        }
    },
    {
        "id": "b19c5228-ed85-4bf9-bc3e-802b9ce6a51e",
        "title": "",
        "chunk_text": "This dynamic, structure-sensitive approach underlies the significant empirical speedups we observe. 25 10 Adaptive Time Complexity and Instance Hardness Analysis A key advantage of our algorithm is its inherent adaptivity: its effective time complexity depends not only on the number of elements n and their bit-length w but also on the structure of the input as captured by U = |Σ(S)|.",
        "metadata": {
            "author": "",
            "keywords": [
                "dynamic",
                "structure-sensitive",
                "observe",
                "Time",
                "Complexity",
                "approach",
                "underlies",
                "significant",
                "empirical",
                "speedups"
            ]
        }
    },
    {
        "id": "f2c4811d-01f5-47b3-bd8e-77c4814f5ddc",
        "title": "",
        "chunk_text": "In our approach, the interplay between dynamic programming and combinatorial enumeration naturally adjusts the computational effort based on the intrinsic characteristics of the input. 10.1 Key Factors Influencing Instance Classification and Hardness Let n denote the number of elements in the set S, and let w = log2(max S) be the bit- length of the largest element.",
        "metadata": {
            "author": "",
            "keywords": [
                "approach",
                "input",
                "interplay",
                "dynamic",
                "programming",
                "combinatorial",
                "enumeration",
                "naturally",
                "adjusts",
                "computational"
            ]
        }
    },
    {
        "id": "573ccbf1-52f0-4cf4-9860-381493550295",
        "title": "",
        "chunk_text": "In addition to the classical measures of additive structure—such as the doubling constant C and additive energy E(S)—we also consider the extended structural parameters introduced earlier: the linearity factor λ, the clustering factor γ, and the duplicate measure δ. These parameters collectively capture the degree of additive redundancy in S and offer a finer-grained understanding of how collisions among subset sums occur.",
        "metadata": {
            "author": "",
            "keywords": [
                "factor",
                "additive",
                "structure",
                "earlier",
                "parameters",
                "measures",
                "measure",
                "addition",
                "classical",
                "doubling"
            ]
        }
    },
    {
        "id": "ecbf91ee-af01-46fd-ae32-01ce1dbad431",
        "title": "",
        "chunk_text": "We can broadly classify instances as follows: Dense Instances: When w is relatively small compared to n (i.e., the elements are confined to a narrow numerical range) and the extended parameters indicate high redun- dancy (for example, low λ and γ or high δ), many subsets yield identical sums. In these cases, the effective search space U = |Σ(S)| is dramatically smaller than 2n.",
        "metadata": {
            "author": "",
            "keywords": [
                "Dense",
                "instances",
                "dancy",
                "high",
                "range",
                "redun",
                "low",
                "sums",
                "broadly",
                "classify"
            ]
        }
    },
    {
        "id": "99739d0a-d8e6-412a-979c-9dc407b4ffd4",
        "title": "",
        "chunk_text": "Sparse Instances: When w is large and the input exhibits little additive structure, the number of unique subset sums U approaches its worst-case bound. Mixed Instances: When S contains regions with differing structural properties, the overall behavior is a blend of the above cases. For mixed instances, the input is partitioned at least into two subsets: SD (the dense part) with nD elements and effective bit-length wD, leading to a smaller UD.",
        "metadata": {
            "author": "",
            "keywords": [
                "Instances",
                "Sparse",
                "structure",
                "bound",
                "input",
                "large",
                "exhibits",
                "additive",
                "number",
                "unique"
            ]
        }
    },
    {
        "id": "865c5f4b-daf5-44e2-8bbc-a4fe10c572ea",
        "title": "",
        "chunk_text": "SS (the sparse part) with nS elements and effective bit-length wS, where US is closer to 2n/2. Naturally, nD + nS = n. 10.2 Time Complexity in Different Regimes The algorithm’s running time adapts to the structure of the instance, which can be broadly categorized into dense, sparse, and mixed regimes. 10.2.1 Dense Regime When the instance is dense, our solver behaves much like a dynamic programming approach on a relatively small numerical range.",
        "metadata": {
            "author": "",
            "keywords": [
                "part",
                "dense",
                "elements",
                "effective",
                "bit-length",
                "closer",
                "Regimes",
                "Time",
                "sparse",
                "instance"
            ]
        }
    },
    {
        "id": "7678a41c-d906-49a9-b246-c5a62196ffc5",
        "title": "",
        "chunk_text": "Because collisions occur early, each column expansion prunes many redundant sums, resulting in a small effective numerical range of unique subset sums (i.e., U ≪2n/2). Furthermore, since we only need to expand up to n 4 elements in each split, the running time is typically bounded by Tdense(n) = O \u0010 n 4 · w \u0011 = O(n w) 26 once collisions saturate the partial sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "early",
                "resulting",
                "collisions",
                "occur",
                "column",
                "expansion",
                "prunes",
                "redundant",
                "small"
            ]
        }
    },
    {
        "id": "34338091-66c7-428f-b72d-7ea0eb04b0af",
        "title": "",
        "chunk_text": "10.2.2 Sparse Regimes When the elements span a large numerical range (so w is comparable to n), collisions are rare, and the algorithm tends to enumerate close to the full meet-in-the-middle bound of e O \u00002n/2\u0001 (i.e., U approaches 2n/2).",
        "metadata": {
            "author": "",
            "keywords": [
                "Sparse",
                "Regimes",
                "range",
                "collisions",
                "rare",
                "full",
                "bound",
                "approaches",
                "elements",
                "span"
            ]
        }
    },
    {
        "id": "5a702fcc-6905-41a0-b58a-7ea5b8bb53a8",
        "title": "",
        "chunk_text": "Nonetheless, duplicates and some inherent structure can still reduce the actual branching factor: Unstructured: In the absence of duplicates and additional structure, the solver reverts to near-worst-case time: Tsparse(n) = e O \u00002n/2\u0001 Structured : In the presence of duplicates, additional structure, and/or clustering the solver effectively reduces the number of unique sums, so that Tsparse_structured,dups,additive,cluster(n, δ, λ, γ) ≈ e O \u0010 2 \u0010 n/2−c δ \u0011 ·λ·γ\u0011 10.2.3 Mixed Regime In mixed instances, where the input contains both dense and sparse segments, the overall running time can be approximated as a weighted sum: Tmixed,dups,additive,cluster(n, δ, λ, γ) ≈fD · O(nD wD) + fS · e O \u0010 2 \u0010 n/2−c δ \u0011 ·λ·γ\u0011 where fD and fS are the fractions of the instance exhibiting dense and sparse behavior, respectively, and the effective number of unique sums U in the dense segment is much smaller than in the sparse segment.",
        "metadata": {
            "author": "",
            "keywords": [
                "Tsparse",
                "duplicates",
                "sparse",
                "structure",
                "dense",
                "Mixed",
                "unique",
                "additional",
                "solver",
                "number"
            ]
        }
    },
    {
        "id": "2ca0bdcf-9e79-4ed7-b117-509fa3bba067",
        "title": "",
        "chunk_text": "10.3 Topological vs. Numerical Target Perspective In a classical sense, one might guess that Σ(S)/2 is the hardest target because enumerating all subsets up to that value can be combinatorially explosive. However, our column-by-column enumerator is structured so that each level of expansion focuses on all subsets (permutations) of size k before moving on to size k + 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "Topological",
                "Target",
                "Numerical",
                "Perspective",
                "sense",
                "explosive",
                "subsets",
                "classical",
                "guess",
                "hardest"
            ]
        }
    },
    {
        "id": "f781455e-fb7b-435f-993f-c524d8984d93",
        "title": "",
        "chunk_text": "Consequently, partial sums near Σ(S)/2 may be discovered (and pruned, if duplicated) relatively early—often via complementary checks around half of ℓ0 and half of ℓ1. By contrast, a target t that requires (for instance) the maximum k-subset expansions (i.e. near k = n/4 per split) is “topologically deeper” in the enumerator’s tree. The solver only finalizes those largest subsets after it has exhausted all smaller-k expansions.",
        "metadata": {
            "author": "",
            "keywords": [
                "half",
                "partial",
                "discovered",
                "pruned",
                "duplicated",
                "early",
                "expansions",
                "sums",
                "complementary",
                "checks"
            ]
        }
    },
    {
        "id": "0bc2039d-65e4-49a8-a8b0-95d8f95f4a1a",
        "title": "",
        "chunk_text": "Hence, from the enumerator’s standpoint, the hardest target is not numerically about being near Σ(S)/2, but rather about being reached only by large or late-expanding subsets. 10.4 Output-Sensitive Perspective A noteworthy aspect of our approach is its output-sensitive nature. In many applications, it is not enough to merely decide whether a target sum t exists; one may need to produce 27 all distinct subset sums (or a representative subset of them).",
        "metadata": {
            "author": "",
            "keywords": [
                "standpoint",
                "enumerator",
                "hardest",
                "numerically",
                "reached",
                "large",
                "late-expanding",
                "target",
                "Perspective",
                "Output-Sensitive"
            ]
        }
    },
    {
        "id": "e41b602d-4674-4e6b-875b-2fa62a288588",
        "title": "",
        "chunk_text": "Because our enumeration process generates each new distinct sum exactly once, the total running time is effectively proportional to the number U of unique sums that are actually enumerated. Formally, if enumerating or listing these distinct sums themselves is viewed as part of the required output, then a runtime of O(U) (or e O(U)) is necessarily optimal from an output-sensitive standpoint. That is, one cannot do better than linear time in the size of the output generated.",
        "metadata": {
            "author": "",
            "keywords": [
                "distinct",
                "enumerated",
                "sums",
                "enumeration",
                "process",
                "generates",
                "total",
                "running",
                "effectively",
                "proportional"
            ]
        }
    },
    {
        "id": "fa585eb6-a385-4f65-850c-ddb179b2fb4d",
        "title": "",
        "chunk_text": "11 Adaptive Runtime Experiments We perform experiments to demonstrate how the cost of generating U depends on various structural properties, and how the adaptive enumeration model self-adjusts during computation to optimally construct the specific combinatorial tree of the input instance. All experiments were executed using the non-anytime version of the algorithm to simplify the runtime analysis to a single cycle.",
        "metadata": {
            "author": "",
            "keywords": [
                "Adaptive",
                "Experiments",
                "Runtime",
                "properties",
                "instance",
                "perform",
                "demonstrate",
                "cost",
                "generating",
                "depends"
            ]
        }
    },
    {
        "id": "9564bde0-c523-4881-91bb-e35cacd85518",
        "title": "",
        "chunk_text": "Additionally, solution finding was disabled to simulate the worst-case scenario—a no-solution instance that requires exhaustive exploration of the problem space. 11.1 Dense Instances In this experiment, we generate a worst-case instance with n = 48 and w = 48 and verify that it indeed produces U = 2n/2 unique subset sums. In this baseline instance, all elements are distinct and sufficiently spread out so that no additional additive structure is exploited.",
        "metadata": {
            "author": "",
            "keywords": [
                "Additionally",
                "instance",
                "solution",
                "scenario",
                "space",
                "worst-case",
                "finding",
                "disabled",
                "simulate",
                "no-solution"
            ]
        }
    },
    {
        "id": "cfc4e27e-bb51-4400-8ef1-8d9f1ce0d33d",
        "title": "",
        "chunk_text": "To ensure the accuracy of the experiment, every element is confirmed to be exactly w bits long. This precaution avoids creating a mixed instance, where certain regions of the input might be denser than others. Figure 4 illustrates the effect of density on the enumeration process. We identify a threshold at approximately w ≈32 (corresponding to a density of 1.5), below which density begins to influence the total number of unique subset sums progressively.",
        "metadata": {
            "author": "",
            "keywords": [
                "experiment",
                "long",
                "ensure",
                "accuracy",
                "element",
                "confirmed",
                "bits",
                "density",
                "instance",
                "Figure"
            ]
        }
    },
    {
        "id": "0d71adcb-5095-43f0-8f34-7cd1b27a07c5",
        "title": "",
        "chunk_text": "We then reduce the bit-length of the elements by 4 bits at each step (effectively shifting the numbers right) to generate instances with higher density. This setup demonstrates the progressive runtime dynamism of the enumerator as the effective search space U is reduced. Figure 4 Counts of k-subsets enumerated per column expansion for a (input n = 48) input instance with elements of varying bit-length from w=32 to w=20 decreasing in steps of 4 bits.",
        "metadata": {
            "author": "",
            "keywords": [
                "bits",
                "effectively",
                "density",
                "reduce",
                "shifting",
                "numbers",
                "generate",
                "higher",
                "bit-length",
                "elements"
            ]
        }
    },
    {
        "id": "c9c47f09-9590-491d-82ec-3da3ea497415",
        "title": "",
        "chunk_text": "28 Scenario Density Unique Subset Sums Diff with Previous baseline (n=48/w=48) 1.0 8 388 607 – n=48/w=32 1.50 8,382,135 99.92% n=48/w=28 1.71 8,311,785 99.16% n=48/w=24 2.00 7,061,331 84.96% n=48/w=20 2.40 2,072,581 29.35% n=48/w=16 3.00 227,034 10.95% Table 1 Comparison of Unique Subset Sums and Density for n = 48 with varying bit-length w. Transition Phase Analysis and Time Complexity Approximation. Let w = log2 \u0010 max x∈S x \u0011 be the bit-length of the elements in the input set S.",
        "metadata": {
            "author": "",
            "keywords": [
                "Subset",
                "Unique",
                "Sums",
                "Density",
                "Table",
                "Scenario",
                "Comparison",
                "Diff",
                "Previous",
                "baseline"
            ]
        }
    },
    {
        "id": "444230a3-f7cb-4051-907b-65ede754dae3",
        "title": "",
        "chunk_text": "In the worst-case scenario—when no additive structure is exploited—the classical meet-in-the-middle algorithm generates roughly 2n/2 distinct subset sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "scenario",
                "exploited",
                "classical",
                "algorithm",
                "roughly",
                "distinct",
                "sums",
                "worst-case",
                "additive",
                "structure"
            ]
        }
    },
    {
        "id": "966acd9e-3a30-42f9-8dfc-954ad6198433",
        "title": "",
        "chunk_text": "However, if the elements in S are small (i.e., w is small), then the total sum of all elements is upper bounded by X x∈S x = O \u0010 n · 2w\u0011 By the pigeonhole principle, even if we consider all possible subsets, the number of distinct subset sums (i.e., the effective search space) is at most U = O \u0010 n · 2w\u0011 Thus, when the input is dense (i.e., when the range of possible sums is much smaller than the number of candidate subsets), many different k-subsets yield the same sum.",
        "metadata": {
            "author": "",
            "keywords": [
                "small",
                "number",
                "elements",
                "subsets",
                "sum",
                "sums",
                "principle",
                "space",
                "dense",
                "total"
            ]
        }
    },
    {
        "id": "ac2ecdaf-67a6-4b8b-9a6a-e091191eb955",
        "title": "",
        "chunk_text": "This phenomenon occurs when n · 2w ≪2n/2 Taking logarithms of both sides gives w < n 2 −log2 n In other words, if the bit-length w of the input elements satisfies w < n 2 −log2 n then the effective search space U is bounded by U = O \u0010 n · 2w\u0011 which is exponentially smaller than 2n/2.",
        "metadata": {
            "author": "",
            "keywords": [
                "Taking",
                "words",
                "phenomenon",
                "occurs",
                "logarithms",
                "sides",
                "bit-length",
                "input",
                "elements",
                "satisfies"
            ]
        }
    },
    {
        "id": "171bf58e-c450-4bc5-96eb-f7afae83803c",
        "title": "",
        "chunk_text": "This threshold marks the transition phase of the enumerator: when w is below this threshold, the algorithm benefits from a high degree of collisions (i.e., many k-subsets yield 29 the same sum), resulting in a dramatic reduction in the number of branches that must be explored as the density increases. Furthermore, since our enumeration model implements a double meet-in-the-middle strategy, the maximum number of iterations is n/4 (as observed in our experimental runtime graphs).",
        "metadata": {
            "author": "",
            "keywords": [
                "threshold",
                "number",
                "enumerator",
                "collisions",
                "yield",
                "sum",
                "resulting",
                "increases",
                "marks",
                "transition"
            ]
        }
    },
    {
        "id": "fe518dfb-2550-441c-ae2e-ff4126402df7",
        "title": "",
        "chunk_text": "Consequently, the overall time complexity of the algorithm improves to roughly O \u0010n 4 · 2w\u0011 ≈O \u0010 n · 2w\u0011 which is exponentially better than the worst-case bound when w < n 2 −log2 n. 11.2 Extremely Dense Instances In this experiment, we process several instances with n = 100 and varying w values (16, 20, and 24), with d > 3.5. In extremely dense instances, the maximum average expansion per element is 2w, and the maximum number of column expansions is n/4.",
        "metadata": {
            "author": "",
            "keywords": [
                "Instances",
                "Dense",
                "Extremely",
                "time",
                "complexity",
                "algorithm",
                "improves",
                "roughly",
                "exponentially",
                "worst-case"
            ]
        }
    },
    {
        "id": "87ce7d70-0366-4f60-a5c9-e1e5b97879bd",
        "title": "",
        "chunk_text": "Figure 5 Counts of k-subsets enumerated per column expansion (input n = 100 with w = 16, 20, 24). In extremely dense instances, the pigeonhole principle forces so many collisions that the enumeration must expand through all required columns, resulting in a runtime dominated by these dense expansions. This outcome is precisely what one would expect from the inherent column-by-column dynamics of our approach.",
        "metadata": {
            "author": "",
            "keywords": [
                "Counts",
                "Figure",
                "input",
                "k-subsets",
                "enumerated",
                "expansion",
                "expansions",
                "dense",
                "instances",
                "resulting"
            ]
        }
    },
    {
        "id": "5c5d819a-b35e-4520-9eb9-0e8c345defa0",
        "title": "",
        "chunk_text": "11.3 Duplicates Elements Figure 6 illustrates the effect of duplicates during enumeration, we start with a worst-case instance of n=48 with 2n/2 unique subset sums (no duplicates) and progressively introduce duplicates, up to 4 per split, duplicating always a different element. 30 Figure 6 Counts of k-subsets enumerated per column expansion (input n = 48).",
        "metadata": {
            "author": "",
            "keywords": [
                "Duplicates",
                "Figure",
                "Elements",
                "element",
                "illustrates",
                "enumeration",
                "unique",
                "sums",
                "split",
                "duplicating"
            ]
        }
    },
    {
        "id": "13aa52ce-e600-4dc7-be0f-faec672383eb",
        "title": "",
        "chunk_text": "We can observe how the most expensive enumeration matches our previous results for worst-case instances with n=48, and how the introduction of a new duplicate reduces the total unique subset sums from the input instance by a 75% from the previous run consistently. Scenario Unique Subset Sums Diff.",
        "metadata": {
            "author": "",
            "keywords": [
                "subset",
                "previous",
                "unique",
                "sums",
                "consistently",
                "observe",
                "expensive",
                "enumeration",
                "matches",
                "results"
            ]
        }
    },
    {
        "id": "a228e3b1-1574-4caf-9235-1a005884456b",
        "title": "",
        "chunk_text": "with Previous No Duplicates 8 388 607 – 1 Duplicate 6 291 455 75% 2 Duplicates 4 718 591 75% 3 Duplicates 3 538 943 75% 4 Duplicates 2 654 207 75% Table 2 Number of unique subset sums for various levels of duplication, and the percentage difference from the previous row.",
        "metadata": {
            "author": "",
            "keywords": [
                "Duplicates",
                "Table",
                "Previous",
                "Number",
                "Duplicate",
                "duplication",
                "row",
                "unique",
                "subset",
                "sums"
            ]
        }
    },
    {
        "id": "27c2d2b4-0a73-45f7-9e09-39d65d2b974a",
        "title": "",
        "chunk_text": "Duplicates Approximate Time Complexity Analysis In the worst-case input instances (with no duplicates and no additional additive structure properties) the number of candidate unique subset sums are U = 2n/2, where n is the input length. Suppose that for each duplicate encountered, the number of effective unique subset sums is reduced by a factor of 0.75 as demonstrated by the experiment. This gives rise to the recurrence T(U, δ + 1) = 0.75 T(U, δ) with the base case T(U, 0) = 2n/2.",
        "metadata": {
            "author": "",
            "keywords": [
                "Approximate",
                "Time",
                "Complexity",
                "Analysis",
                "input",
                "number",
                "subset",
                "unique",
                "sums",
                "instances"
            ]
        }
    },
    {
        "id": "fdd254e3-11dd-422b-8f38-24bf520a4bcf",
        "title": "",
        "chunk_text": "Solving this recurrence, we obtain T(U, δ) = 2n/2 · (0.75)δ Since 0.75 = 2−log2(4/3) and log2(4/3) ≈0.415, 31 we can express the reduction factor as (0.75)δ = 2−0.415 δ Thus, the overall time complexity becomes T(U, δ) = 2n/2−0.415 δ This shows that, with δ duplicates, the effective number of branches is reduced by a factor of approximately 2−0.415 δ, leading to a final complexity of eΘ \u0010 2n/2−0.415 δ\u0011 when a varying number of elements have a single duplicate present on its corresponding split.",
        "metadata": {
            "author": "",
            "keywords": [
                "number",
                "factor",
                "complexity",
                "Solving",
                "recurrence",
                "approximately",
                "leading",
                "split",
                "obtain",
                "express"
            ]
        }
    },
    {
        "id": "23c494e1-d8e9-4175-8c68-4ff9716a68df",
        "title": "",
        "chunk_text": "11.4 Additive Structure In this experiment, we process an instance with n = 48 and introduce arithmetic progressions to observe the effect of additive structure on the number of unique subset sums. First, we insert one arithmetic progression (a sequence) of 3 elements and then extend it to 4 elements. Next, we introduce a disjoint progression to form two sequences—first with 3 elements each, and then extend both sequences to 4 elements each.",
        "metadata": {
            "author": "",
            "keywords": [
                "Additive",
                "Structure",
                "elements",
                "arithmetic",
                "experiment",
                "sums",
                "progression",
                "process",
                "instance",
                "observe"
            ]
        }
    },
    {
        "id": "9650941c-771d-497c-9dea-27ee2670c455",
        "title": "",
        "chunk_text": "This experimental design allows us to verify the effect of having one versus two sequences per split, as well as the impact of increasing the sequence length, on the reduction of unique subset sums. Figure 7 Counts of k-subsets enumerated per column expansion (input n = 48) when the input instance includes 1 or 2 arithmetic progressions of 3 and 4 elements.",
        "metadata": {
            "author": "",
            "keywords": [
                "split",
                "length",
                "sums",
                "experimental",
                "design",
                "verify",
                "effect",
                "versus",
                "impact",
                "increasing"
            ]
        }
    },
    {
        "id": "5c40778e-b2fd-45f2-a92a-55c4b4849a03",
        "title": "",
        "chunk_text": "Scenario Unique Subset Sums Diff with Previous No sequences (2n/2) 8 388 607 – 1 seq – 3 length 7 340 031 87.50% 1 seq – 4 length 5 767 167 78.57% No sequences (2n/2) 8 388 607 – 2 seq – 3 length 6 422 527 76.56% 2 seq – 4 length 3 964 927 61.73% Table 3 Unique Subset Sums and percentage difference for the one and two sequence cases. 32 We observe that the presence of two sequences reduces the number of unique subset sums more than a single sequence does.",
        "metadata": {
            "author": "",
            "keywords": [
                "seq",
                "length",
                "Subset",
                "Table",
                "Unique",
                "Sums",
                "Diff",
                "Previous",
                "sequences",
                "Scenario"
            ]
        }
    },
    {
        "id": "d6514b69-b1ba-42fa-9fb0-a782a8095e26",
        "title": "",
        "chunk_text": "Furthermore, extending the sequence length by one element results in an additional reduction of approximately 9% for the one-sequence case and about 15% for the two-sequence case. This demonstrates that the effect of additive structure compounds, thereby reducing the total number of unique subset sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "case",
                "extending",
                "approximately",
                "sequence",
                "length",
                "element",
                "results",
                "additional",
                "reduction",
                "one-sequence"
            ]
        }
    },
    {
        "id": "d2161f79-d7f1-46a2-bfd0-f1a28dbc248f",
        "title": "",
        "chunk_text": "Additional experiments with other types of sequences (e.g., arithmetic progressions, quadratic sequences, and certain Fibonacci-like sequences) confirm that structured sequences with near-linear or progression-like behavior can significantly reduce the total number of unique subset sums below 2n/2..",
        "metadata": {
            "author": "",
            "keywords": [
                "sequences",
                "Fibonacci-like",
                "Additional",
                "arithmetic",
                "progressions",
                "quadratic",
                "confirm",
                "experiments",
                "types",
                "structured"
            ]
        }
    },
    {
        "id": "0f98fa36-92ad-451b-82c0-1b06f677bcbb",
        "title": "",
        "chunk_text": "11.5 Summary The conducted experiments demonstrates that adaptive behavior of our algorithm is directly tied to the effective search space U which is dependent on the structural characteristics of the input instance. In particular, the overall runtime improves when the input instance exhibits structural characteristics that reduce U relative to the worst-case bound.",
        "metadata": {
            "author": "",
            "keywords": [
                "Summary",
                "input",
                "structural",
                "characteristics",
                "instance",
                "conducted",
                "experiments",
                "demonstrates",
                "adaptive",
                "behavior"
            ]
        }
    },
    {
        "id": "d996c419-e153-4ed9-8b4e-6e1f78f5673f",
        "title": "",
        "chunk_text": "For example, when the instance is: Dense: When the elements are confined to a narrow numerical range (i.e., small bit- length w), many subset sums collide, yielding a significantly reduced U and near-dynamic programming performance (O(n w)). Redundant (Duplicates): The presence of duplicate elements inherently reduces the number of distinct subset sums by eliminating redundant branches.",
        "metadata": {
            "author": "",
            "keywords": [
                "Dense",
                "range",
                "small",
                "bit",
                "length",
                "collide",
                "yielding",
                "performance",
                "subset",
                "sums"
            ]
        }
    },
    {
        "id": "4e6e3aa9-a969-4d7f-9945-5a4724a9895d",
        "title": "",
        "chunk_text": "Structured (Linearity and Clustering): When the elements exhibit near-linear relationships or are tightly clustered, the resulting additive structure forces many k- subsets to produce the same sum, thereby reducing U. In contrast, in sparse instances where the elements are widely spread and lack significant structure, U remains close to the worst-case 2n/2, and the algorithm reverts to near-worst- case performance.",
        "metadata": {
            "author": "",
            "keywords": [
                "Linearity",
                "Clustering",
                "Structured",
                "elements",
                "clustered",
                "subsets",
                "sum",
                "structure",
                "exhibit",
                "near-linear"
            ]
        }
    },
    {
        "id": "f337e6e8-1ced-4509-b7d6-ddaeb3f92945",
        "title": "",
        "chunk_text": "For mixed instances, the overall runtime reflects a combination of these behaviors. This refined understanding of instance hardness—captured by U and influenced by factors such as duplicates, linearity, and clustering—enables our solver to gracefully adapt to the specific characteristics of the input without requiring explicit control flow adjustments, specialized pre-processing, or prior knowledge of the instance’s structure. Concluding Remark on Adaptive Hardness.",
        "metadata": {
            "author": "",
            "keywords": [
                "hardness",
                "behaviors",
                "instance",
                "mixed",
                "runtime",
                "reflects",
                "combination",
                "linearity",
                "Remark",
                "Adaptive"
            ]
        }
    },
    {
        "id": "f9ccac8a-eb29-4e5b-b265-ef7ca80ee4ab",
        "title": "",
        "chunk_text": "Our experimental and theoretical analysis confirms that the effective search space U = |Σ(S)| is intricately governed by the input’s additive structure—whether measured through small doubling, high additive energy, specialized sequence patterns, or duplicate redundancy. These combinatorial properties directly dictate the number of unique subset sums that our algorithm must process, and thus, they are the key to its adaptive pruning strategy and overall speedup.",
        "metadata": {
            "author": "",
            "keywords": [
                "additive",
                "structure",
                "doubling",
                "high",
                "energy",
                "specialized",
                "patterns",
                "redundancy",
                "experimental",
                "theoretical"
            ]
        }
    },
    {
        "id": "f7054632-491d-4015-af75-5bb6c9eae626",
        "title": "",
        "chunk_text": "This unified perspective not only explains the observed transition from worst-case to near-dynamic programming performance in structured instances but also provides a clear roadmap for further optimizing NP-complete solvers by focusing on the inherent structure of the input. 33 12 Combining All the Optimizations Our algorithm integrates several key techniques to reduce the effective work required for solving Subset Sum, even in worst-case instances.",
        "metadata": {
            "author": "",
            "keywords": [
                "input",
                "worst-case",
                "instances",
                "unified",
                "perspective",
                "explains",
                "observed",
                "transition",
                "near-dynamic",
                "programming"
            ]
        }
    },
    {
        "id": "494aaa9c-b4b8-4a33-af03-c7cdf119a5be",
        "title": "",
        "chunk_text": "We start with an original time complexity bound based on the enumeration of unique subset sums: T(n) = O \u0010 poly(n) · \u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0011 which, after suppressing polylogarithmic factors, can be written as T(n) ≈e O \u0010\u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0011 Using Stirling’s approximation (and ignoring the floor functions for large n), we set: N = n 2 −n 16 = 7n 16 and K = n 4 −n 32 = 7n 32 Since K/N = 1 2, the binomial coefficient is approximated by \u0012N K \u0013 ≈e O \u0010 2N·H(1/2)\u0011 = e O \u0010 2N\u0011 = e O \u0010 27n/16\u0011 Incorporating Additional Optimizations 1.",
        "metadata": {
            "author": "",
            "keywords": [
                "Incorporating",
                "Optimizations",
                "Stirling",
                "Additional",
                "poly",
                "sums",
                "factors",
                "approximation",
                "set",
                "start"
            ]
        }
    },
    {
        "id": "e703e536-e57a-495d-a90e-c23b871aa602",
        "title": "",
        "chunk_text": "Complement Trick: Instead of enumerating all subsets in each half (which would be 2n/2), we generate only subsets up to size n/4, since any larger subset is the complement of a smaller one. This effectively reduces the count by a factor of 2, so that in each half we have about 27n/16 2 2. Collision Injection: By forcing one duplicate per half, we introduce a collision that further reduces the number of unique subset sums by a factor of approximately 0.75.",
        "metadata": {
            "author": "",
            "keywords": [
                "Trick",
                "Complement",
                "half",
                "factor",
                "enumerating",
                "generate",
                "size",
                "larger",
                "smaller",
                "reduces"
            ]
        }
    },
    {
        "id": "5ac987d2-c266-422d-8ca3-7109c9c44b6f",
        "title": "",
        "chunk_text": "Combining with the complement trick, the effective number of enumerated subsets becomes: \u001227n/16 2 \u0013 × 0.75 ≈27n/16−1.415 where the constant 1.415 arises from the logarithm in base 2 of the constant factor 2 × 1 0.75−1 ≈2.67 (since 2−1.415 ≈0.375). 3. Anytime (Cyclic) Processing: The enumeration is organized into approximately n/4 cycles, where each subsequent cycle performs roughly half the work of the previous one.",
        "metadata": {
            "author": "",
            "keywords": [
                "constant",
                "Combining",
                "trick",
                "arises",
                "base",
                "factor",
                "complement",
                "effective",
                "number",
                "enumerated"
            ]
        }
    },
    {
        "id": "2e191401-1093-4808-b4d8-4bd04422dcd7",
        "title": "",
        "chunk_text": "This geometric decay sums to only a constant multiple of the work done in the first cycle and thus does not change the exponential term.",
        "metadata": {
            "author": "",
            "keywords": [
                "term",
                "geometric",
                "decay",
                "sums",
                "constant",
                "multiple",
                "work",
                "cycle",
                "change",
                "exponential"
            ]
        }
    },
    {
        "id": "c8de0e3f-6340-43e2-978c-00819aa31c03",
        "title": "",
        "chunk_text": "Overall Worst-Case Time Complexity and Speedup In the worst-case, the classical meet-in-the-middle algorithm runs in time: TMIM(n) = e O \u0010 2n/2\u0011 = e O \u0010 28n/16\u0011 34 By combining the complement trick, collision injection, and cyclic processing, our algorithm achieves an effective worst-case time complexity of: Tours(n) ≈e O \u0010 27n/16−1.415\u0011 Thus, the speedup factor relative to the classical approach is: 28n/16 27n/16−1.415 = 2 n 16 +1.415 This means that even in the worst-case, where the effective search space is nearly 2n/2, our optimizations yield an exponential speedup of approximately 2n/16 (with an additional constant factor of roughly 21.415), or better if the combinatorial tree is compressed more.",
        "metadata": {
            "author": "",
            "keywords": [
                "Time",
                "Worst-Case",
                "Speedup",
                "Complexity",
                "TMIM",
                "Tours",
                "classical",
                "algorithm",
                "effective",
                "factor"
            ]
        }
    },
    {
        "id": "834b1694-0cc6-40f0-af4f-51830b954bae",
        "title": "",
        "chunk_text": "Although our algorithm still operates in exponential time in the worst-case, the combined techniques ensure that it always enumerates strictly fewer than 2n/2 subsets.",
        "metadata": {
            "author": "",
            "keywords": [
                "subsets",
                "worst-case",
                "algorithm",
                "operates",
                "exponential",
                "time",
                "combined",
                "techniques",
                "ensure",
                "enumerates"
            ]
        }
    },
    {
        "id": "583afe88-3043-4848-b381-5f59544e3da4",
        "title": "",
        "chunk_text": "Table 4 summarizes the effects of the discussed optimizations: Technique Multiplicative Factor Exponent Reduction (bits) Effective Complexity Classical MIM 1 0 e O(2n/2) Half-Subset Enumeration 1/2 1 e O(2n/2−1) Collision Injection 0.75 log2(1/0.75) ≈0.415 e O(2n/2−1.415) Anytime/Online – n/16 + 1.415 e O \u0010 27n/16−1.415\u0011 Table 4 Summary of how each optimization step reduces the effective time complexity exponent.",
        "metadata": {
            "author": "",
            "keywords": [
                "Table",
                "Anytime",
                "Online",
                "Exponent",
                "Effective",
                "Complexity",
                "Technique",
                "Reduction",
                "MIM",
                "Enumeration"
            ]
        }
    },
    {
        "id": "3d191cc7-33ea-4656-8e9f-8ef24087c7a5",
        "title": "",
        "chunk_text": "13 Comparison with Existing Solvers Table 5 summarizes how traditional state-of-the-art approaches compare with the proposed adaptive, structure-aware solver. 35 Algorithm / Ap- proach Worst-Case Time Memory Structure Use? Anytime- Online?",
        "metadata": {
            "author": "",
            "keywords": [
                "Comparison",
                "Table",
                "Existing",
                "Algorithm",
                "summarizes",
                "traditional",
                "approaches",
                "adaptive",
                "structure-aware",
                "Online"
            ]
        }
    },
    {
        "id": "063dc39a-cbaf-4d59-9bc1-5ab3579bf307",
        "title": "",
        "chunk_text": "Key Charac- teristics Limitations Dynamic Pro- gramming (Bellman)[9] O(n · t) (pseudo- polynomial) O(t) Minimal No Exact if t is small; conceptu- ally simple Infeasible if t large Koiliaris–Xu (2017)[10] e O(t√n) O(t) Partial (fast convolution in DP) No Pseudopolynomial FFT-based method for Subset Sum Performance tied to t; not fully structure- adaptive Schroeppel– Shamir (1979)[23] e O(2n/2) e O(2n/4) None No Reduced mem- ory vs.",
        "metadata": {
            "author": "",
            "keywords": [
                "Bellman",
                "Partial",
                "Shamir",
                "Charac",
                "Pro",
                "Minimal",
                "Koiliaris",
                "Schroeppel",
                "gramming",
                "pseudo"
            ]
        }
    },
    {
        "id": "a6eabd2f-3e2b-456e-8d4e-95b1f3870103",
        "title": "",
        "chunk_text": "classi- cal MIM (BFS merge of partial sums) Still 2n/2 in time; no struc- tural adaptivity; more complex to implement Classical Meet- in-the-Middle (Horowitz & Sahni)[1] e O(2n/2) e O(2n/2) None No Deterministic splitting and merging Worst-case exponential; sorting/merging overhead Randomized Methods (e.g.",
        "metadata": {
            "author": "",
            "keywords": [
                "Horowitz",
                "Sahni",
                "classi",
                "sorting",
                "MIM",
                "BFS",
                "Meet",
                "Methods",
                "Classical",
                "Deterministic"
            ]
        }
    },
    {
        "id": "8ebfe768-0367-47ee-a973-1a6c39eecc30",
        "title": "",
        "chunk_text": "Howgrave– Graham & Joux)[8, 2] Often sub-2n/2 on average Similar to MIM (e O(2n/2)) Partial (prob- abilistic colli- sions) No Often faster in practice; ex- ploits collisions probabilistically No exact guaran- tee; performance variability Recent De- terministic Improvements (e.g.",
        "metadata": {
            "author": "",
            "keywords": [
                "Graham",
                "Joux",
                "Partial",
                "Howgrave",
                "prob",
                "sions",
                "tee",
                "MIM",
                "Improvements",
                "Similar"
            ]
        }
    },
    {
        "id": "987dc0bf-b019-4bd1-8422-33562800b67f",
        "title": "",
        "chunk_text": "Bringmann, Nederlof et al.)[11, 12] e O(2n/2) Varies (often exponential) Some structure usage No Improved bounds in certain regimes Still 2n/2 worst- case; less dy- namic adaptiv- ity Adaptive, Structure- Aware Solver (Proposed) e O(2n/2) worst- case; effectively e O(U) in struc- tured cases Reduced vs.",
        "metadata": {
            "author": "",
            "keywords": [
                "worst",
                "Varies",
                "Proposed",
                "Nederlof",
                "Adaptive",
                "Aware",
                "Solver",
                "Bringmann",
                "structure",
                "Improved"
            ]
        }
    },
    {
        "id": "c7e19a40-3dd2-46ce-b57d-b97ee823b915",
        "title": "",
        "chunk_text": "MIM; e O(U) if U ≪2n/2 Yes (collision pruning, dupli- cations, etc.) Yes (incremen- tal & online) Dynamic collision- pruning; enumerates only distinct sums; double MIM + DP Exponential if no struc- ture; but real speedups in practice Table 5 Comparison of various Subset Sum solvers with the proposed adaptive, structure-aware approach.",
        "metadata": {
            "author": "",
            "keywords": [
                "dupli",
                "cations",
                "MIM",
                "pruning",
                "Dynamic",
                "Table",
                "Comparison",
                "incremen",
                "tal",
                "online"
            ]
        }
    },
    {
        "id": "7499432e-2998-4117-a711-456bcb64b30c",
        "title": "",
        "chunk_text": "14 Key Insights and Benefits In addition to the core complexity improvements and adaptive features described in previous sections, our algorithm exhibits several further properties that enhance its practical and theoretical appeal. Many of these benefits stem from our focus on the effective search space—namely, the unique subset sums, whose total number is U = |Σ(S)|. We summarize these properties below.",
        "metadata": {
            "author": "",
            "keywords": [
                "Key",
                "Insights",
                "Benefits",
                "sections",
                "appeal",
                "addition",
                "core",
                "complexity",
                "improvements",
                "adaptive"
            ]
        }
    },
    {
        "id": "5fe13ded-240e-4110-8ece-f238895ebc89",
        "title": "",
        "chunk_text": "14.1 Anytime and Incremental Behavior Our algorithm’s slicing and rescheduling techniques enable it to produce intermediate results during the enumeration process. This anytime behavior is particularly valuable in 36 time-critical or resource-constrained environments, as it allows the algorithm to be interrupted at any point while still providing partial, useful outputs and an exact solution upon full execution.",
        "metadata": {
            "author": "",
            "keywords": [
                "Incremental",
                "process",
                "Anytime",
                "Behavior",
                "algorithm",
                "slicing",
                "rescheduling",
                "techniques",
                "enable",
                "produce"
            ]
        }
    },
    {
        "id": "9a5b4829-2845-42e6-9183-32268f992013",
        "title": "",
        "chunk_text": "The incremental nature of the combinatorial tree exploration, which tracks only unique subset sums (i.e., up to U distinct sums), ensures that progress is steadily made toward a solution. 14.2 Online Updates as a New Branch Beyond the anytime behavior described in Section 8, our solver can also operate online, handling newly arrived elements without discarding previous partial enumerations.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "exploration",
                "ensures",
                "solution",
                "incremental",
                "nature",
                "combinatorial",
                "tree",
                "tracks",
                "unique"
            ]
        }
    },
    {
        "id": "33fce427-efb2-4d23-a94b-3c1cd0223b81",
        "title": "",
        "chunk_text": "In practice, adding a new element xnew to one of the splits (ℓ0 or ℓ1) is treated exactly like introducing a new “branch” in the enumerator: 1. We record xnew in the relevant split’s data structure and update the total sum of that split accordingly. 2. The algorithm’s standard column-expansion process (including any look-ahead or slicing rules) then applies to xnew just as it would to any other element.",
        "metadata": {
            "author": "",
            "keywords": [
                "branch",
                "xnew",
                "practice",
                "adding",
                "enumerator",
                "split",
                "treated",
                "introducing",
                "element",
                "splits"
            ]
        }
    },
    {
        "id": "a7ab640e-45a3-480d-84df-a4037c6bd926",
        "title": "",
        "chunk_text": "Consequently, xnew will contribute to new k-permutations, potentially introducing additional unique subset sums. Importantly, the memoization mechanism continues to track all previously generated unique sums (i.e., the current state of U), ensuring that no work is lost. Because we do not discard or rebuild the enumerations performed so far, all prior informa- tion—including the unique subset sums—remains intact.",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "unique",
                "xnew",
                "k-permutations",
                "potentially",
                "subset",
                "contribute",
                "introducing",
                "additional",
                "Importantly"
            ]
        }
    },
    {
        "id": "dd512e68-144c-4a50-b549-b6904b421831",
        "title": "",
        "chunk_text": "The new element’s presence is seamlessly incorporated as if it had been present from the start, with only the new branches (and the corresponding updates to U) undergoing the enumeration steps. 14.3 Adaptive Prioritization and Governability A distinctive benefit of our anytime exploration is that it renders the search process highly governable.",
        "metadata": {
            "author": "",
            "keywords": [
                "start",
                "branches",
                "undergoing",
                "steps",
                "element",
                "presence",
                "seamlessly",
                "incorporated",
                "present",
                "updates"
            ]
        }
    },
    {
        "id": "3efacd82-f1a0-423b-afe1-32b05db61eda",
        "title": "",
        "chunk_text": "Since the algorithm operates in configurable discrete cycles—each producing a set of candidate branches that contribute to U—we can reassess the search state after every cycle. Once a cycle completes, the scheduler can reorganize the rescheduled branches by prioritizing those likely to yield new unique subset sums, pruning those that are redundant (i.e., those whose sums already exist in Σ(S)), and even deliberately triggering collisions to block unpromising regions of the search space.",
        "metadata": {
            "author": "",
            "keywords": [
                "search",
                "branches",
                "cycle",
                "algorithm",
                "operates",
                "configurable",
                "discrete",
                "producing",
                "set",
                "candidate"
            ]
        }
    },
    {
        "id": "465095b8-93fc-4856-8849-2312f0a0e979",
        "title": "",
        "chunk_text": "This adaptive prioritization not only guarantees the delivery of partial solutions at any interruption point but also enables the solver to be controlled and fine-tuned in real time, enhancing overall efficiency. 14.4 Early Capture of Imbalanced Solutions Any solution that requires an imbalanced distribution of elements between the two splits is typically detected earlier in the enumeration process.",
        "metadata": {
            "author": "",
            "keywords": [
                "time",
                "enhancing",
                "efficiency",
                "solutions",
                "Imbalanced",
                "adaptive",
                "prioritization",
                "guarantees",
                "delivery",
                "partial"
            ]
        }
    },
    {
        "id": "883ec8e3-07f5-479f-a347-87b7a6ec1711",
        "title": "",
        "chunk_text": "In our algorithm, the worst-case scenario arises when ≈n/4 elements are required from at least one split. In contrast, if a valid solution exists that uses fewer than n/4 elements from one split, it will be found in an earlier cycle. The more imbalanced the solution (i.e., the greater the difference in the number of elements chosen from each split), the sooner its corresponding unique subset sum is generated, leading to faster detection of a solution.",
        "metadata": {
            "author": "",
            "keywords": [
                "split",
                "elements",
                "algorithm",
                "solution",
                "worst-case",
                "scenario",
                "arises",
                "required",
                "contrast",
                "cycle"
            ]
        }
    },
    {
        "id": "64e0d070-016f-4c00-844e-b8a104d73dca",
        "title": "",
        "chunk_text": "37 14.5 Potential for Integration with Other Heuristics The structural design of our algorithm facilitates the integration of additional heuristics or approximation techniques. In scenarios where an exact solution is not strictly necessary, heuristic methods may be incorporated into the combinatorial tree exploration or the dynamic programming phase to further accelerate computation.",
        "metadata": {
            "author": "",
            "keywords": [
                "Integration",
                "Potential",
                "techniques",
                "Heuristics",
                "structural",
                "design",
                "algorithm",
                "facilitates",
                "additional",
                "approximation"
            ]
        }
    },
    {
        "id": "573f65a9-74b7-482c-a661-5e45c429d392",
        "title": "",
        "chunk_text": "By leveraging the reduced effective search space U, such hybrid approaches can balance exactness with speed, broadening the practical utility of our framework. 14.6 Approximation via Bit Clearing and Rounding Error Bound One further advantage of our approach is the ability to trade a small amount of accuracy for significant efficiency gains.",
        "metadata": {
            "author": "",
            "keywords": [
                "speed",
                "broadening",
                "framework",
                "leveraging",
                "reduced",
                "effective",
                "search",
                "space",
                "hybrid",
                "approaches"
            ]
        }
    },
    {
        "id": "4fd26642-6790-4577-8138-082ce8ccf8af",
        "title": "",
        "chunk_text": "By clearing a fixed number of bits from the input values and the target, we reduce the effective bit-length, effectively pushing the problem into a denser regime where collisions (and hence a smaller U) occur more frequently. Although this introduces a bounded rounding error, it substantially reduces the computational load without compromising overall accuracy. 14.7 Parallelizability The inherent structure of our algorithm naturally lends itself to parallel processing.",
        "metadata": {
            "author": "",
            "keywords": [
                "target",
                "bit-length",
                "effectively",
                "collisions",
                "occur",
                "frequently",
                "clearing",
                "fixed",
                "number",
                "bits"
            ]
        }
    },
    {
        "id": "9ee290fe-bd7f-47d5-b3a6-62627e87fde1",
        "title": "",
        "chunk_text": "The division of the input into two independent splits (via the double meet-in-the-middle approach) and the independent processing of different slices of the combinatorial tree create multiple subproblems that can be handled concurrently. As each subproblem is focused on generating a portion of the unique subset sums (contributing to U), the workload can be distributed efficiently across multiple cores or nodes, further reducing practical running times.",
        "metadata": {
            "author": "",
            "keywords": [
                "independent",
                "approach",
                "splits",
                "double",
                "concurrently",
                "multiple",
                "division",
                "input",
                "processing",
                "slices"
            ]
        }
    },
    {
        "id": "90d2e100-236f-4fa8-b3ba-1428ba3acecb",
        "title": "",
        "chunk_text": "14.8 Framework for Recasting Other Problems The approach of constructing a combinatorial tree for unique subset sum generation, combined with complement-based matching and dynamic programming integration, provides a versatile framework. This framework is not limited to the Subset Sum problem; it can be generalized to tackle other NP-hard combinatorial problems that exhibit similar structural properties.",
        "metadata": {
            "author": "",
            "keywords": [
                "Recasting",
                "subset",
                "sum",
                "Framework",
                "generation",
                "combined",
                "integration",
                "Problems",
                "combinatorial",
                "approach"
            ]
        }
    },
    {
        "id": "c6d2b32c-2433-4613-a87b-f38382676d1d",
        "title": "",
        "chunk_text": "By focusing on the effective search space U rather than the worst-case 2n bound, our method paves the way for new algorithmic paradigms that are both structure-aware and practically efficient. 14.9 Space Complexity While our algorithm retains the same worst-case time complexity as the classical meet-in- the-middle approach, i.e., e O \u0012\u0012n/2 n/4 \u0013\u0013 ≈e O \u0010 2n/2\u0011 in practice it requires less than half the memory in the worst-case.",
        "metadata": {
            "author": "",
            "keywords": [
                "worst-case",
                "space",
                "bound",
                "efficient",
                "Complexity",
                "focusing",
                "effective",
                "search",
                "method",
                "paves"
            ]
        }
    },
    {
        "id": "97440152-b9c7-4c3e-a1c3-70ab41123b94",
        "title": "",
        "chunk_text": "By focusing on storing only the unique subset sums (of effective size U), the memory footprint is significantly reduced in all instances. 38 15 Emergent Dynamism and Instance Hardness A feature of our algorithm is its ability to adapt dynamically to the structure of the input by focusing on the effective search space—namely, the unique subset sums (U = |Σ(S)|).",
        "metadata": {
            "author": "",
            "keywords": [
                "sums",
                "focusing",
                "unique",
                "subset",
                "effective",
                "storing",
                "size",
                "memory",
                "footprint",
                "significantly"
            ]
        }
    },
    {
        "id": "939d3ad3-436e-4522-b30e-af5dd188011a",
        "title": "",
        "chunk_text": "Without requiring any global knowledge of the complete combinatorial tree, the algorithm incrementally extends partial k-permutations only when the resulting k-subset sum is unique, thereby keeping U as small as possible. 15.1 Instance Hardness Classification The effective complexity of an instance is determined by the number of unique subset sums U, rather than by the total number of subsets.",
        "metadata": {
            "author": "",
            "keywords": [
                "tree",
                "unique",
                "Instance",
                "requiring",
                "global",
                "knowledge",
                "complete",
                "combinatorial",
                "algorithm",
                "incrementally"
            ]
        }
    },
    {
        "id": "006943ed-cc76-4e7f-856e-8e11f99cdc14",
        "title": "",
        "chunk_text": "In practice: Hard Instances are those for which U is close to the worst-case 2n/2, necessitating nearly full exploration. However, when the combinatorial tree is compressed, by injecting collisions in the format of one or two forced duplicates (as in Section 7), even these “worst-case” inputs are forcibly compressed below 2n/2 unique sums, ensuring that the effective enumeration remains strictly sub-2n/2.",
        "metadata": {
            "author": "",
            "keywords": [
                "Hard",
                "Instances",
                "worst-case",
                "practice",
                "necessitating",
                "exploration",
                "close",
                "full",
                "Section",
                "compressed"
            ]
        }
    },
    {
        "id": "c812aec7-e9e7-49fd-b68f-8cb82066d95c",
        "title": "",
        "chunk_text": "sub-2n/2 Instances exhibit inherent structure (e.g., duplicates, near-linearity, or clus- tering) that naturally produces far fewer distinct sums, allowing extensive pruning even without explicit modifications.",
        "metadata": {
            "author": "",
            "keywords": [
                "duplicates",
                "near-linearity",
                "tering",
                "Instances",
                "structure",
                "clus",
                "sums",
                "allowing",
                "modifications",
                "exhibit"
            ]
        }
    },
    {
        "id": "a82e8240-b3fa-49cf-8315-f613b2c6ca12",
        "title": "",
        "chunk_text": "In short, while the naive worst-case exponential cost eO(2n/2) arises when an input lacks any detectable structure, many real instances—and indeed any instance combinatorial tree augmented with injected collisions—demonstrate considerably smaller U, and thus a significantly reduced effective search space. 15.2 Scaling Invariance A notable property is that the exploration cost, as measured by U, remains invariant under uniform scaling of the input elements and the target.",
        "metadata": {
            "author": "",
            "keywords": [
                "short",
                "arises",
                "structure",
                "collisions",
                "demonstrate",
                "space",
                "cost",
                "input",
                "Scaling",
                "naive"
            ]
        }
    },
    {
        "id": "d5f061f8-4ce1-49b4-9faa-7f2d709cb66d",
        "title": "",
        "chunk_text": "Thus, if the input is scaled by a constant factor, the relative structure—and hence the effective U—remains unchanged. This highlights that instance hardness is governed by combinatorial structure rather than absolute numerical values. 16 Implications for NP-Complete Problems and Future Directions This adaptive, structure-aware approach to Subset Sum has implications that extend beyond a single solver.",
        "metadata": {
            "author": "",
            "keywords": [
                "factor",
                "remains",
                "unchanged",
                "structure",
                "input",
                "scaled",
                "constant",
                "relative",
                "effective",
                "Implications"
            ]
        }
    },
    {
        "id": "f75c4f9d-5591-45bb-8630-2a28d29fd3e0",
        "title": "",
        "chunk_text": "It sheds light on both the nature of instance-specific hardness and potential lines of attack for (or barriers to) improving worst-case bounds in NP-complete problems. 16.1 Broader Significance for NP-Complete Problems Intrinsic Hardness: The minimal computation cost needed to expand the unique subset sums (i.e., to increase U) provides an instance-specific measure of hardness.",
        "metadata": {
            "author": "",
            "keywords": [
                "problems",
                "hardness",
                "improving",
                "NP-complete",
                "instance-specific",
                "sheds",
                "light",
                "nature",
                "potential",
                "lines"
            ]
        }
    },
    {
        "id": "9158521d-36ca-4e80-81b4-771c40d495fb",
        "title": "",
        "chunk_text": "This effectively shifts the perspective from a blanket 2n (or 2n/2) worst-case assumption to the actual number of distinct sums that materialize for a given input. Reframing Open Problems: Improving upon the eO(2n/2) bound (in a deterministic sense) would entail reducing the “effective search space” U as well as compressing the 39 process needed to enumerate or detect those sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "blanket",
                "worst-case",
                "input",
                "sums",
                "effectively",
                "shifts",
                "perspective",
                "assumption",
                "actual",
                "number"
            ]
        }
    },
    {
        "id": "d01270f3-6550-4b41-92cf-fa8348955fb1",
        "title": "",
        "chunk_text": "The collision-driven enumeration approach clarifies that if U truly remains large, even an optimal pruning strategy still faces exponential growth.",
        "metadata": {
            "author": "",
            "keywords": [
                "large",
                "growth",
                "collision-driven",
                "enumeration",
                "approach",
                "clarifies",
                "remains",
                "optimal",
                "pruning",
                "strategy"
            ]
        }
    },
    {
        "id": "c56c32aa-782f-4e30-bfff-564dc74b422d",
        "title": "",
        "chunk_text": "(Techniques such as our forced-duplicate method illustrate that such compression is feasible in practice, though generalizing these methods to broader settings remains an open research challenge.) Guiding Future Research: By focusing on the effective search space U, future work might explore hybrid strategies, potentially merging collision-based pruning with other algorithmic or combinatorial optimizations.",
        "metadata": {
            "author": "",
            "keywords": [
                "Techniques",
                "practice",
                "challenge",
                "research",
                "Future",
                "forced-duplicate",
                "illustrate",
                "compression",
                "feasible",
                "generalizing"
            ]
        }
    },
    {
        "id": "4147d265-aa11-4ade-a72f-2a6b8012b07b",
        "title": "",
        "chunk_text": "This instance-centric perspective could inform new heuristics, partial dynamic programming methods, or specialized branch-and-bound approaches in broader NP-hard problems. 16.2 Discussion, Lower-Bound Perspectives, and Future Directions While our primary focus is on designing a practical and adaptive solver, the underlying unique-subset-sums enumerator has additional theoretical ramifications: Potential for Real-Time Structural Insights.",
        "metadata": {
            "author": "",
            "keywords": [
                "Discussion",
                "heuristics",
                "partial",
                "methods",
                "specialized",
                "approaches",
                "problems",
                "Potential",
                "Insights",
                "instance-centric"
            ]
        }
    },
    {
        "id": "03905d36-bf23-45eb-9918-613116f182e9",
        "title": "",
        "chunk_text": "Our method prunes branches immediately when a newly formed partial sum collides with a previously generated sum, effectively measuring the collision rate in real time. This rate correlates directly with key structural parameters, such as the additive energy and doubling constant of S. A high early collision rate indicates substantial redundancy or near-linearity within the set, while infrequent collisions suggest that the instance is approaching the worst- case 2n/2 behavior.",
        "metadata": {
            "author": "",
            "keywords": [
                "sum",
                "rate",
                "effectively",
                "time",
                "method",
                "prunes",
                "branches",
                "immediately",
                "newly",
                "formed"
            ]
        }
    },
    {
        "id": "81ddfbe3-7745-4f98-b930-3715f4cbd844",
        "title": "",
        "chunk_text": "Moreover, the litmus test described in Section 5.4 runs in polynomial time, offering an efficient means to approximate U and assess the intrinsic structural hardness of the input instance. Instance-Focused Hardness. Our approach underpins an instance-based notion of hardness: even though Subset Sum is NP-complete, the difficulty for any particular input is driven by the effective number of distinct sums U = |Σ(S)| rather than by n alone.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "hardness",
                "runs",
                "time",
                "offering",
                "instance",
                "litmus",
                "test",
                "polynomial",
                "efficient"
            ]
        }
    },
    {
        "id": "5afab234-81d3-44aa-a422-489794ca0e0c",
        "title": "",
        "chunk_text": "This shifts the complexity lens from a purely worst-case statement (2n/2 or bust) to an adaptive metric. We thus envision new analyses of why certain inputs remain Ω(2n/2) even for an “optimal” collision-pruning algorithm. These analyses could reinforce conditional lower bounds: if even a collision-driven enumerator must enumerate nearly 2n/2 partial sums in a particular input family, that family embodies a near-worst-case instance for all algorithms of this style.",
        "metadata": {
            "author": "",
            "keywords": [
                "statement",
                "bust",
                "metric",
                "shifts",
                "complexity",
                "lens",
                "purely",
                "worst-case",
                "adaptive",
                "analyses"
            ]
        }
    },
    {
        "id": "d4d22359-1d69-4785-bb32-d119ca202c4d",
        "title": "",
        "chunk_text": "Possible Paths to Lower Bounds. Although unconditional lower bounds for Subset Sum remain elusive, our enumerator— being near-optimal in enumerating unique sums—could serve as a framework for showing that no deterministic approach can do better on carefully constructed input families. If such a family provably forces Σ(S) to be large, then even an anytime, collision-aware search must handle an exponential cascade of distinct partial sums.",
        "metadata": {
            "author": "",
            "keywords": [
                "Paths",
                "Lower",
                "Bounds",
                "sums",
                "Subset",
                "elusive",
                "enumerator",
                "families",
                "large",
                "anytime"
            ]
        }
    },
    {
        "id": "556cc97f-c254-4f52-b3b0-92679264ec06",
        "title": "",
        "chunk_text": "This could support new “fine-grained” or “instance-based” hardness proofs, tying enumerator-level arguments directly to structured combinatorial lower-bound research. While fully closing the gap to a strong unconditional bound is beyond current methods, we believe this line of investigation will shed fresh light on why certain Subset Sum instances remain genuinely hard despite the many structural shortcuts discussed in this paper. 40 Future Work.",
        "metadata": {
            "author": "",
            "keywords": [
                "fine-grained",
                "instance-based",
                "hardness",
                "proofs",
                "tying",
                "research",
                "support",
                "enumerator-level",
                "arguments",
                "directly"
            ]
        }
    },
    {
        "id": "666d7689-f8e1-4d1b-8214-e14c557aac10",
        "title": "",
        "chunk_text": "Real-time structure detection: Integrate collision-rate tracking and partial-sum distribution plots to classify inputs automatically. Broader NP-hard problems: Adapt our enumerator to other combinatorial domains where partial states often coincide, e.g., knapsack variants, integer linear programs with repeated coefficients, or specialized SAT instances with symmetrical clauses.",
        "metadata": {
            "author": "",
            "keywords": [
                "Integrate",
                "Real-time",
                "detection",
                "automatically",
                "Adapt",
                "structure",
                "collision-rate",
                "tracking",
                "partial-sum",
                "distribution"
            ]
        }
    },
    {
        "id": "8976ec57-44cb-4760-9096-eeee320c0dc1",
        "title": "",
        "chunk_text": "Refined lower-bound frameworks: Investigate families of sets that remain intractable for any collision-based algorithm, thus supporting stronger lower-bound arguments in a fine-grained or parameterized sense. Altogether, we view the enumerator as not only a powerful practical tool but also a lens for examining deeper additive-structure phenomena and for guiding future complexity-theoretic lines of inquiry.",
        "metadata": {
            "author": "",
            "keywords": [
                "Investigate",
                "lower-bound",
                "Refined",
                "frameworks",
                "algorithm",
                "sense",
                "families",
                "sets",
                "remain",
                "intractable"
            ]
        }
    },
    {
        "id": "d0deaa8a-8ec3-4b93-ae62-2e52d568ba13",
        "title": "",
        "chunk_text": "17 Conclusions We have investigated a novel solver for the Subset Sum problem, improving on the classical meet-in-the-middle algorithm of Horowitz and Sahni [1] in several concrete ways. Our approach is built around the observation that, in practice, the true computational challenge lies in generating only the unique subset sums (i.e., the effective search space of size U = |Σ(S)|), which is typically much smaller than the worst-case 2n possibilities.",
        "metadata": {
            "author": "",
            "keywords": [
                "Conclusions",
                "Sahni",
                "Horowitz",
                "Subset",
                "problem",
                "improving",
                "classical",
                "algorithm",
                "investigated",
                "solver"
            ]
        }
    },
    {
        "id": "aba06af9-c9c9-4ef7-ab19-c322008a2b0a",
        "title": "",
        "chunk_text": "Specifically: Adaptive Behavior Across Instances. As analyzed in Section 10, our solver adapts smoothly to the structure of the input. In dense instances—where high collision rates yield a small U—the algorithm achieves near-dynamic programming performance, while in sparse instances it gracefully reverts to near-worst-case behavior. This instance-dependent adaptivity, measured effectively by U, leads to significant practical speedups on many inputs. Reduced Enumeration Per Split.",
        "metadata": {
            "author": "",
            "keywords": [
                "Adaptive",
                "Specifically",
                "Instances",
                "Behavior",
                "Section",
                "Split",
                "Enumeration",
                "input",
                "inputs",
                "performance"
            ]
        }
    },
    {
        "id": "ba7f90bb-39a7-4ea6-998a-a8aad0bf61ff",
        "title": "",
        "chunk_text": "By enumerating only subsets up to size n/4 in each half, the algorithm systematically captures all “larger” subsets via arithmetic complements. This strategy effectively reduces the number of explicit enumerations—generating roughly 2n/2−1 subsets per half instead of 2n/2—and limits the unique sums produced to U, the much smaller effective space. Elimination of Sorting/Merging Phases.",
        "metadata": {
            "author": "",
            "keywords": [
                "larger",
                "subsets",
                "complements",
                "half",
                "enumerating",
                "size",
                "algorithm",
                "systematically",
                "captures",
                "arithmetic"
            ]
        }
    },
    {
        "id": "b3fb37c6-244d-4569-8219-0362dbec1ae8",
        "title": "",
        "chunk_text": "Rather than creating two massive lists of subset sums that require sorting (with a cost of e O(n 2n/2)), our method immediately verifies each newly generated subset sum via constant-time hash lookups. This immediate checking directly leverages the fact that only new unique sums (contributing to U) are kept, thus eliminating the expensive sorting step. Anytime and Online Capabilities.",
        "metadata": {
            "author": "",
            "keywords": [
                "subset",
                "lookups",
                "sums",
                "sorting",
                "creating",
                "massive",
                "lists",
                "require",
                "cost",
                "method"
            ]
        }
    },
    {
        "id": "51f309e0-aadb-462b-8600-283c87cd7a37",
        "title": "",
        "chunk_text": "The slicing and rescheduling mechanism ensures that intermediate unique subset sums are retained, allowing the algorithm to be paused or updated online without discarding the work already done. New elements can be incor- porated seamlessly, with the memoization structure continuously tracking the evolving set Σ(S) (and hence U). Adaptive Approximation.",
        "metadata": {
            "author": "",
            "keywords": [
                "retained",
                "allowing",
                "slicing",
                "rescheduling",
                "mechanism",
                "ensures",
                "intermediate",
                "unique",
                "subset",
                "sums"
            ]
        }
    },
    {
        "id": "a3afe134-fd50-4def-b211-5badebc6c1a7",
        "title": "",
        "chunk_text": "By clearing a fixed number of significant bits from both the input values and the target, the algorithm reduces the effective numerical resolution, thereby decreasing the effective U through increased collisions. This controlled approxi- mation introduces only a bounded additive error (within ±2N−1, N=number of cleared 41 bits) while substantially reducing computational workload. Governing the Search Process.",
        "metadata": {
            "author": "",
            "keywords": [
                "effective",
                "number",
                "target",
                "resolution",
                "collisions",
                "bits",
                "clearing",
                "fixed",
                "significant",
                "input"
            ]
        }
    },
    {
        "id": "c63cb1b3-9cbd-4774-8abd-b15db694644f",
        "title": "",
        "chunk_text": "Thanks to its anytime, incremental design, the algorithm provides rich intermediate data that can be used to govern subsequent search decisions. After each cycle, the scheduler can dynamically reorganize branches—pruning those that do not yield new unique subset sums (i.e., that do not increase U) and focusing on promising regions of the search space. This fine-grained governability enhances efficiency in later cycles.",
        "metadata": {
            "author": "",
            "keywords": [
                "anytime",
                "incremental",
                "design",
                "decisions",
                "search",
                "algorithm",
                "rich",
                "intermediate",
                "data",
                "govern"
            ]
        }
    },
    {
        "id": "d517afbb-1ca7-465a-94f4-cb23a8dd6671",
        "title": "",
        "chunk_text": "A central feature of our work is the emerging dynamism of the algorithm—its ability to dynamically adjust to different input types by focusing on the effective unique subset sums (U). This adaptability is crucial for handling real-world problems, where data can vary widely in terms of density, structure, and complexity.",
        "metadata": {
            "author": "",
            "keywords": [
                "algorithm",
                "sums",
                "central",
                "feature",
                "work",
                "emerging",
                "dynamism",
                "ability",
                "dynamically",
                "adjust"
            ]
        }
    },
    {
        "id": "0ce88153-fb86-46ea-b83a-05d37f44fe5e",
        "title": "",
        "chunk_text": "The algorithm essentially “learns” the structure of the input and tailors its exploration accordingly, making it more efficient and capable of tackling previously intractable instances. This adaptive nature directly translates to our algorithm’s practical impact: while the worst-case theoretical complexity remains e O(2n/2), our method achieves significant improvements in real-world applications by exploiting structural properties that reduce U.",
        "metadata": {
            "author": "",
            "keywords": [
                "learns",
                "essentially",
                "making",
                "instances",
                "algorithm",
                "structure",
                "input",
                "tailors",
                "exploration",
                "efficient"
            ]
        }
    },
    {
        "id": "e1976032-b48b-42ec-9d88-6dfcd2d0bc0d",
        "title": "",
        "chunk_text": "This work thus bridges theoretical insights from additive combinatorics with practical algorithm design, demonstrating that the most efficient solutions to NP-complete problems may emerge not from universally reducing worst-case complexity but by adapting the solution approach to the problem’s inherent characteristics. Broader Theoretical Implications. Beyond these direct algorithmic contributions, our collision-based enumerator sheds light on deeper instance-focused hardness questions.",
        "metadata": {
            "author": "",
            "keywords": [
                "theoretical",
                "design",
                "demonstrating",
                "characteristics",
                "Implications",
                "work",
                "bridges",
                "insights",
                "additive",
                "combinatorics"
            ]
        }
    },
    {
        "id": "a668ed21-cb51-447f-b351-99661b8fcf61",
        "title": "",
        "chunk_text": "Because each newly formed partial sum is immediately compared against a memoization table, the enumerator effectively tracks the collision rate of sums in real time. High collision rates reveal hidden additive structure—such as small doubling constants or significant repetition—while sparse collisions indicate behavior approaching the worst-case 2n/2 scenario.",
        "metadata": {
            "author": "",
            "keywords": [
                "table",
                "time",
                "collision",
                "newly",
                "formed",
                "partial",
                "immediately",
                "compared",
                "memoization",
                "enumerator"
            ]
        }
    },
    {
        "id": "e7114fa0-1786-43c3-af10-d3edb846fdb0",
        "title": "",
        "chunk_text": "Consequently, the enumerator can diagnose or classify inputs by their additive complexity without a separate preprocessing step; in fact, the litmus test described in Section 5.4 runs in polynomial time, offering an efficient approximation of U and a clear measure of an instance’s intrinsic hardness.",
        "metadata": {
            "author": "",
            "keywords": [
                "Section",
                "step",
                "fact",
                "runs",
                "time",
                "offering",
                "hardness",
                "enumerator",
                "diagnose",
                "classify"
            ]
        }
    },
    {
        "id": "ccd8f1f3-f9eb-4b11-ab01-0a64649a331d",
        "title": "",
        "chunk_text": "From a theoretical perspective, an “optimal” collision-driven enumerator that produces every distinct sum exactly once can serve as a litmus test for potential lower bounds: if even such an approach must explore Ω(2n/2) partial sums for certain constructed families, it strongly implies that no faster deterministic method exists for those instances.",
        "metadata": {
            "author": "",
            "keywords": [
                "optimal",
                "perspective",
                "collision-driven",
                "bounds",
                "partial",
                "families",
                "instances",
                "theoretical",
                "enumerator",
                "produces"
            ]
        }
    },
    {
        "id": "80333c49-448c-471e-ba38-47b722213512",
        "title": "",
        "chunk_text": "Such analyses could bolster instance-based or conditional lower-bound arguments, reinforcing the notion that some Subset Sum instances remain inherently hard despite any structural advantages. Combinatorial Tree Compression and “Hard” Instances. Even for “unstructured” or worst-case inputs—those that otherwise require enumerating up to 2n/2 subsets per half—two straightforward optimizations guarantee a strictly sub-2n/2 enumeration.",
        "metadata": {
            "author": "",
            "keywords": [
                "Sum",
                "arguments",
                "reinforcing",
                "advantages",
                "instances",
                "hard",
                "analyses",
                "bolster",
                "instance-based",
                "conditional"
            ]
        }
    },
    {
        "id": "6b7a6026-51b8-4f32-97fa-569898648c90",
        "title": "",
        "chunk_text": "First, we enumerate only half the subsets per split (those up to size n/4, with the others taken as complements). This alone multiplies the naïve 2n/2 by a factor of 0.5. Second, we force duplicates in each half, which empirically prunes an additional 25% (factor 0.75) per duplicate in each split. Thus, with one duplicate per split, the total search drops to 0.5 × 0.75 = 0.375 42 meaning only about 37.5% of the naïve enumeration remains.",
        "metadata": {
            "author": "",
            "keywords": [
                "split",
                "complements",
                "enumerate",
                "subsets",
                "size",
                "half",
                "naïve",
                "factor",
                "duplicate",
                "multiplies"
            ]
        }
    },
    {
        "id": "da168a55-e2f4-4ac5-ae01-5ac8546c6f5e",
        "title": "",
        "chunk_text": "For two forced duplicates per split, it becomes 0.5 × 0.752 ≈0.28 and so on. Hence, a modest injection of collisions ensures a real constant-factor speedup for every instance, with each additional duplicate further reducing the exponent (up to the point where overhead from extra checks outweighs the benefit). 17.1 Open Directions and Future Research Our work opens several promising avenues for further exploration and enhancement.",
        "metadata": {
            "author": "",
            "keywords": [
                "split",
                "forced",
                "Directions",
                "Future",
                "Research",
                "instance",
                "exponent",
                "benefit",
                "duplicates",
                "duplicate"
            ]
        }
    },
    {
        "id": "534d25da-cd02-4e8c-ac60-bce5f70b6507",
        "title": "",
        "chunk_text": "In addition to the directions discussed above, we identify the following research paths as particularly interesting: Tighter Parameter Bounds. Investigate improved theoretical bounds for the effective search space U using advanced techniques from additive combinatorics. Refining the analysis of parameters such as the doubling constant C and additive energy E(S) may yield a more precise characterization of instance hardness. Hybrid Randomized-Deterministic Algorithms.",
        "metadata": {
            "author": "",
            "keywords": [
                "Tighter",
                "Bounds",
                "interesting",
                "addition",
                "directions",
                "discussed",
                "identify",
                "research",
                "paths",
                "additive"
            ]
        }
    },
    {
        "id": "07c0f778-4fb0-4c0b-a307-2339aee81714",
        "title": "",
        "chunk_text": "Explore the design of algorithms that blend deterministic guarantees with randomized components. Such hybrid methods could leverage the best aspects of both paradigms, achieving improved average-case performance while still ensuring exact solutions. Parallel and Distributed Implementations. While our approach is inherently parallelizable, further exploration of parallel and distributed algorithms is warranted.",
        "metadata": {
            "author": "",
            "keywords": [
                "Explore",
                "components",
                "Distributed",
                "design",
                "blend",
                "deterministic",
                "guarantees",
                "randomized",
                "Parallel",
                "algorithms"
            ]
        }
    },
    {
        "id": "3371213d-4c8f-4a7e-bc63-5b4936922c59",
        "title": "",
        "chunk_text": "Investigating dynamic load balancing, distributed hash table implementations, and fault tolerance mechanisms could lead to additional speedups on modern high-performance computing platforms. Generalization to Other NP-Hard Problems. The framework of unique-sum enumeration, combined with complement-based matching and dynamic slicing, may be adapted to other NP-hard combinatorial problems (e.g., knapsack variants, partition, scheduling, or cryptanalysis).",
        "metadata": {
            "author": "",
            "keywords": [
                "Problems",
                "Investigating",
                "balancing",
                "distributed",
                "implementations",
                "platforms",
                "load",
                "hash",
                "table",
                "fault"
            ]
        }
    },
    {
        "id": "8094e8ad-3d30-4c9b-af44-c82429caae29",
        "title": "",
        "chunk_text": "Studying these generalizations could pave the way for a broader class of structure-aware exact solvers. Improved Data Structures. Explore the integration of advanced data structures—such as succinct data structures, cache-efficient hash tables, or probabilistic filters—to further reduce memory usage and accelerate lookups during the enumeration and dynamic programming phases. Hybrid Heuristic Strategies.",
        "metadata": {
            "author": "",
            "keywords": [
                "Structures",
                "Data",
                "Studying",
                "solvers",
                "generalizations",
                "pave",
                "broader",
                "class",
                "structure-aware",
                "exact"
            ]
        }
    },
    {
        "id": "7541c89b-c71a-4cdc-9c8d-0ca61bf98fc6",
        "title": "",
        "chunk_text": "Investigate the potential of incorporating additional heuristic methods (e.g., greedy or local search techniques) into the unified method framework. Such strategies might further reduce the number of cycles required to compress the search space in practice. Refined Complexity Classification. Study the implications of our adaptive approach on the complexity landscape of NP-complete problems.",
        "metadata": {
            "author": "",
            "keywords": [
                "Investigate",
                "greedy",
                "techniques",
                "framework",
                "search",
                "methods",
                "method",
                "potential",
                "incorporating",
                "additional"
            ]
        }
    },
    {
        "id": "e9aa4f00-a085-4c87-8072-7c8c5b3d3c2f",
        "title": "",
        "chunk_text": "In particular, investigate whether leveraging the concept of the effective search space U and real-time collision metrics can yield conditional lower bounds or a refined classification of problem hardness. Final Remarks.",
        "metadata": {
            "author": "",
            "keywords": [
                "investigate",
                "hardness",
                "Remarks",
                "leveraging",
                "concept",
                "effective",
                "search",
                "space",
                "real-time",
                "collision"
            ]
        }
    },
    {
        "id": "b6b98980-c8c6-4702-8170-2acc6973e893",
        "title": "",
        "chunk_text": "Our solver demonstrates that even without breaking the fundamental e O(2n/2) barrier, a careful recombination of enumerator slicing, dynamic collision-based 43 pruning, partial dynamic programming, and a double meet-in-the-middle strategy, com- bined with a highly effective enumeration model, can yield impressive practical gains.",
        "metadata": {
            "author": "",
            "keywords": [
                "barrier",
                "pruning",
                "strategy",
                "dynamic",
                "slicing",
                "collision-based",
                "partial",
                "programming",
                "double",
                "bined"
            ]
        }
    },
    {
        "id": "3ba6248f-c253-4e88-9f4e-0f6dc161f7a3",
        "title": "",
        "chunk_text": "By automatically adapting to the structure of the input—reflected in the effective number of unique subset sums U—and offering anytime/online capabilities, our method presents a versatile improvement over classical approaches. We hope these ideas not only accelerate practical Subset Sum instances but also inspire deeper investigation into input structures, leading to refined classifications of instance hardness and novel insights into lower bounds for NP-complete problems.",
        "metadata": {
            "author": "",
            "keywords": [
                "subset",
                "reflected",
                "anytime",
                "online",
                "capabilities",
                "approaches",
                "input",
                "automatically",
                "adapting",
                "effective"
            ]
        }
    },
    {
        "id": "ae67f848-c2ed-44f9-843e-c2e4d3b96cb4",
        "title": "",
        "chunk_text": "44 References 1 Ellis Horowitz and Sartaj Sahni. Computing partitions with applications to the knapsack problem. Journal of the ACM, 21(2):277–292, 1974. 2 Karl Bringmann and Vasileios Nakos. Top-k-convolution and the quest for near-linear output-sensitive subset sum. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, page 982–995. Association for Computing Machinery, 2020. 3 Tim Randolph and Karol Węgrzycki.",
        "metadata": {
            "author": "",
            "keywords": [
                "References",
                "Ellis",
                "Sahni",
                "Horowitz",
                "Sartaj",
                "Computing",
                "ACM",
                "STOC",
                "Karl",
                "Nakos"
            ]
        }
    },
    {
        "id": "441c2cf9-de60-4c73-adac-be1e71dd2fee",
        "title": "",
        "chunk_text": "Parameterized Algorithms on Integer Sets with Small Doubling: Integer Programming, Subset Sum and k-SUM. In 32nd Annual European Symposium on Algorithms (ESA 2024), volume 308 of Leibniz International Proceedings in Informatics (LIPIcs), pages 96:1–96:19, 2024. 4 C. Cheeseman, B. Kanefsky, and W. M. Taylor. Where the REALLY hard problems are. In Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI), pages 331–337, 1991. 5 J. C. Lagarias and A. M. Odlyzko.",
        "metadata": {
            "author": "",
            "keywords": [
                "Integer",
                "Doubling",
                "Programming",
                "Subset",
                "Sets",
                "Small",
                "Sum",
                "Algorithms",
                "ESA",
                "Parameterized"
            ]
        }
    },
    {
        "id": "341e30a9-c5dd-4f09-8c86-bbe8a251a34c",
        "title": "",
        "chunk_text": "Solving low-density subset sum problems. J. ACM, 32(1):229–246, 1985. 6 M. J. Coster, A. Joux, B. A. Lamacchia, A. M. Odlyzko, C.-P. Schnorr, and J. Stern. Improved low-density subset sum algorithms. Computational Complexity, 2:111–128, 1992. 7 A. Flaxman and B. Przydatek. Solving medium-density subset sum problems in expected polynomial time. In STACS 2005, volume 3404 of LNCS, pages 305–314. Springer, 2005. 8 Nick Howgrave-Graham and Antoine Joux. New generic algorithms for hard knapsacks.",
        "metadata": {
            "author": "",
            "keywords": [
                "subset",
                "sum",
                "Joux",
                "ACM",
                "Solving",
                "low-density",
                "problems",
                "Coster",
                "Lamacchia",
                "Odlyzko"
            ]
        }
    },
    {
        "id": "df1c7252-9773-4d13-acb6-a9e83d39066f",
        "title": "",
        "chunk_text": "In Henri Gilbert, editor, Advances in Cryptology – EUROCRYPT 2010, pages 235–256. Springer Berlin Heidelberg, 2010. 9 Richard Bellman. The theory of dynamic programming. Bulleting of the American Mathe- matical Society, 60(6):503–515, November 1954. 10 Konstantinos Koiliaris and Chao Xu. A faster pseudopolynomial time algorithm for subset sum. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’17, pages 1062–1072, Philadelphia, PA, USA, 2017.",
        "metadata": {
            "author": "",
            "keywords": [
                "EUROCRYPT",
                "Gilbert",
                "Advances",
                "Cryptology",
                "Henri",
                "editor",
                "pages",
                "Heidelberg",
                "Berlin",
                "Richard"
            ]
        }
    },
    {
        "id": "5a06f9df-6496-4a82-a984-3a3fcaad1aa7",
        "title": "",
        "chunk_text": "Society for Industrial and Applied Mathematics. 11 Karl Bringmann. A near-linear pseudopolynomial time algorithm for subset sum. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1073–1084. SIAM, 2017. 12 Jesper Nederlof and Karol Węgrzycki. Improving schroeppel and shamir’s algorithm for subset sum via orthogonal vectors. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1670–1683. ACM, 2021.",
        "metadata": {
            "author": "",
            "keywords": [
                "Mathematics",
                "Industrial",
                "Applied",
                "pages",
                "Proceedings",
                "Society",
                "Symposium",
                "Karl",
                "Bringmann",
                "Annual"
            ]
        }
    },
    {
        "id": "22c73a40-a382-4940-8054-4d008b02f8ba",
        "title": "",
        "chunk_text": "13 Amir Abboud, Karl Bringmann, Danny Hermelin, and Dvir Shabtay. Seth-based lower bounds for subset sum and bicriteria path. In ACM Transactions on Algorithms (TALG), volume 18, pages 1–22, 2023. 14 Gregory A. Freiman. On the addition of finite sets. Doklady Akademii Nauk, 158:1038–1041, 1964. 15 Pekka Orponen, Ker-I Ko, Uwe Schöning, and Osamu Watanabe. Instance complexity. Journal of the ACM, 41(1):96–121, 1994. 16 G. A. Freiman. The addition of finite sets. i.",
        "metadata": {
            "author": "",
            "keywords": [
                "Amir",
                "Abboud",
                "Karl",
                "Bringmann",
                "Danny",
                "Hermelin",
                "Shabtay",
                "Dvir",
                "Freiman",
                "TALG"
            ]
        }
    },
    {
        "id": "bb72084e-4931-4b4a-87c5-7c52ef909cc0",
        "title": "",
        "chunk_text": "Izvestiya Vysshikh Uchebnykh Zavedenii. Matematika, (6):202–213, 1959. 17 Terence Tao. Structure and randomness in combinatorics, 2007. 18 Terence Tao and Van Vu. Additive Combinatorics. Cambridge University Press, 2006. 19 Imre Z. Ruzsa. Sumsets and structure. In Combinatorial Number Theory and Additive Group Theory, pages 87–210. Birkhäuser, 2009. 20 Giorgis Petridis. New proofs of plünnecke-type estimates for product sets in groups. Combinatorica, 32(6):721–733, 2012. Published 2012/12/01.",
        "metadata": {
            "author": "",
            "keywords": [
                "Zavedenii",
                "Vysshikh",
                "Uchebnykh",
                "Terence",
                "Tao",
                "Izvestiya",
                "combinatorics",
                "Theory",
                "Additive",
                "Matematika"
            ]
        }
    },
    {
        "id": "f640ca42-ca3c-4ca9-bd8f-d11db5887e1c",
        "title": "",
        "chunk_text": "45 21 R. Morris, W. Samotij, and D. Saxton. An asymmetric container lemma and the structure of graphs with no induced 4-cycle. ArXiv preprint arXiv:1806.03706, 2018. 22 David Saxton and Andrew Thomason. Hypergraph containers. Inventiones mathematicae, 201(3):925–992, January 2015. 23 Richard Schroeppel and Adi Shamir. A o(2n/2,s = o(2n/4)) algorithm for certain np-complete problems. SIAM Journal on Computing, 10(3):456–464, 1981.",
        "metadata": {
            "author": "",
            "keywords": [
                "Saxton",
                "Samotij",
                "Morris",
                "ArXiv",
                "January",
                "David",
                "Thomason",
                "Richard",
                "Shamir",
                "Andrew"
            ]
        }
    },
    {
        "id": "e32bc14f-8fa5-42fe-bf2d-ffe877e46d1f",
        "title": "",
        "chunk_text": "46 A Unique Subset sums Enumerator Algorithm Algorithm 1 Unique Subset sums enumerator 1: struct kPerm(sum, elements) { ▷holds a k-permutation instance 2: sum ←0 ▷sum of elements 3: elements[n] ←{0, 0, ...} ▷elements of this k-permutation 4: } 5: 6: S ←{...} 7: n ←len(S) 8: MemoizedSums ←{} 9: uniqueSubsetSumDictionary ←{} 10: 11: Input ←{kP erm(0, {})} 12: 13: procedure Enumerate 14: for i ←0 to n/2 do ▷for all columns 15: Output ←{} ▷Clean the Output 16: for all current ∈Input do ▷for all k-permutations on Input 17: for k ←0 to n −1 do ▷for all elements in S 18: if current.elements[k] = 0 then ▷if element is not included 19: sum ←current.sum + S[k] ▷expand 20: if MemoizedSums.lookup(sum) = 0 then ▷not found?",
        "metadata": {
            "author": "",
            "keywords": [
                "sum",
                "Input",
                "Subset",
                "Output",
                "Unique",
                "Algorithm",
                "elements",
                "Enumerator",
                "sums",
                "Enumerate"
            ]
        }
    },
    {
        "id": "6a5efbc9-41b5-4181-9e12-5ba3885f44fc",
        "title": "",
        "chunk_text": "21: MemoizedSums.add(sum) ←1 ▷set as found 22: expanded ←new kP erm(sum, current.elements) ▷copy current 23: expanded.elements[k] ←1 ▷flag new element as included 24: Output.add(expanded) ▷to the Output list 25: uniqueSubsetSumDictionary.Add(sum, expanded) ▷track k-subsets 26: Input ←Output ▷prepare Input for next column expansion 47 Algorithm 2 Subset Sum Solver with Double Meet-in-the-Middle Optimization 1: struct kPerm(sum, elements, split) { 2: elements[n] ←{0, 0, . .",
        "metadata": {
            "author": "",
            "keywords": [
                "expanded",
                "sum",
                "Output",
                "Input",
                "Output.add",
                "Algorithm",
                "Optimization",
                "Subset",
                "Double",
                "Solver"
            ]
        }
    },
    {
        "id": "116d1163-e4bf-4646-8be5-25194fc73362",
        "title": "",
        "chunk_text": ".} ▷indicator vector for inclusion 3: split ←−1 ▷split membership (0 for ℓ0, 1 for ℓ1) 4: sum ←0 ▷sum of elements 5: } 6: ℓ0 ←{} ▷left split 7: ℓ1 ←{} ▷right split 8: MemoizedSumsL0 ←{} ▷Unique sums from ℓ0 9: MemoizedSumsL1 ←{} ▷Unique sums from ℓ1 10: allKSubsetsL0 ←{} ▷k-subsets from ℓ0 11: allKSubsetsL1 ←{} ▷k-subsets from ℓ1 12: SumL0 ←0 ▷Total sum of elements in ℓ0 13: SumL1 ←0 ▷Total sum of elements in ℓ1 14: procedure Initialize(S) 15: for k ←0 to n −1 do 16: if k mod 2 = 0 then 17: ℓ0.add(S[k]) 18: SumL0 ←SumL0 + S[k] 19: else 20: ℓ1.add(S[k]) 21: SumL1 ←SumL1 + S[k] 22: Input.Add(kPerm(0, {}, 0)) ▷empty k-permutation for left split 23: Input.Add(kPerm(0, {}, 1)) ▷empty k-permutation for right split 24: procedure Solver(S, target) 25: Initialize(S) 26: for i ←0 to n 4 −1 do ▷iterate for each column expansion (up to n/2 per split) 27: Output ←{} 28: for all current ∈Input do 29: splitElements ←ℓ0 if current.split = 0, ℓ1 otherwise 30: MemoizedSums ← n MemoizedSumsL0, if current.split = 0 MemoizedSumsL1, if current.split = 1 31: allKSubsets ← n allKSubsetsL0, if current.split = 0 allKSubsetsL1, if current.split = 1 32: theOtherKSubsets ← n allKSubsetsL1, if current.split = 0 allKSubsetsL0, if current.split = 1 33: theOtherMemoizedSums ← n MemoizedSumsL1, if current.split = 0 MemoizedSumsL0, if current.split = 1 34: currentSplitSum ← n SumL0, if current.split = 0 SumL1, if current.split = 1 35: for k ←1 to len(splitElements) do 36: if current.elements[k] = 0 then ▷if element S[k] not yet included 37: sum ←current.sum + splitElements[k] 38: if MemoizedSums.lookup(sum) = 0 then 39: MemoizedSums.add(sum) ←1 40: expanded ←new kP erm(sum, current.elements, current.split) 41: expanded.elements[k] ←1 42: Output.add(expanded) 43: allKSubsets.add(sum, expanded) ▷Compute and store the complementary subset sum: 44: MemoizedSums.add(currentSplitSum −sum) ←1 45: allKSubsets.add(currentSplitSum −sum, expanded) 46: Check(expanded, target, theOtherKSubsets, theOtherMemoizedSums, currentSplitSum) 47: Input ←Output 48: print No solution found’ 49: procedure Check(kSubset, target, theOtherKSubsets, theOtherMemoizedSums, currentSplitSum) 50: if kSubset.sum = target then ▷current sum = t?",
        "metadata": {
            "author": "",
            "keywords": [
                "current.split",
                "sum",
                "currentSplitSum",
                "expanded",
                "split",
                "target",
                "Output",
                "splitElements",
                "theOtherKSubsets",
                "theOtherMemoizedSums"
            ]
        }
    },
    {
        "id": "3123e9a0-1fe1-49bb-8c38-c4ecbc9feab7",
        "title": "",
        "chunk_text": "51: print “Subset from a single Split : ” + kSubset 52: STOP 53: if (currentSplitSum −kSubset.sum) = target then ▷k-subsetc = t? 54: print “Subset found on a single Split using a complement subset:” + kSubset 55: STOP 56: if theOtherMemoizedSums[ target −kSubset.sum ] = 1 then 57: theOther ←theOtherKSubsets[target −kSubset.sum] 58: if theOther.sum = target −kSubset.sum then ▷is theOther B or Bc?",
        "metadata": {
            "author": "",
            "keywords": [
                "STOP",
                "print",
                "kSubset",
                "kSubset.sum",
                "target",
                "Split",
                "Subset",
                "currentSplitSum",
                "k-subsetc",
                "single"
            ]
        }
    },
    {
        "id": "ae8fc056-e720-446d-9fc5-d47f402e810c",
        "title": "",
        "chunk_text": "59: composedSubset ←kSubset.Combine(theOther) ▷Σ(A ∈ℓx) + Σ(B ∈ℓy) = t 60: print “(A+B) subsets sum found using both splits:” + composedSubset 61: else 62: composedSubset ←kSubset.Combine(theOther.Complement()) ▷Σ(A ∈ℓx) + Σ(Bc ∈ℓy) = t 63: print “(A+B’) subsets sum found using both splits:” + composedSubset 64: STOP 65: if theOtherMemoizedSums[ target −(currentSplitSum −kSubset.sum) ] = 1 then 66: composedSubset ← kSubset.Combine(theOtherKSubsets[ target − (currentSplitSum − kSubset.sum) ]) 67: print “(A’+B’) subsets sum found using both splits:” + composedSubset.Complement() 68: STOP 48 Algorithm 3 Divide-and-Conquer Procedure (Anytime) 1: 2: [...] 3: // The following queues must be processed in order until depleted 4: schedulingQueues ←{{P erm(0, {}, 0), KP erm(0, {}, 1)}, {}, ..., {}} 5: lookAhead ←n/16 ▷Empirically determined 6: [...] 7: 8: procedure ExpandOrSchedule(kP erm, splitElements, splitMemoizedSums, currentCycle) 9: mustExpand ←T rue 10: kIndex ←kP erm.LastDeferralIndex + 1 11: currentStep ←0 12: while currentStep < lookAhead AND mustExpand = T rue do 13: kSubsetSum ←kP erm.sum + splitElements[kIndex] 14: if kIndex < len(splitElements) AND kP erm.elements[kIndex] = 0 then ▷Element not yet included 15: if splitMemoizedSums.lookup(kSubsetSum) = 1 then ▷Sum already found?",
        "metadata": {
            "author": "",
            "keywords": [
                "composedSubset",
                "STOP",
                "kSubset.Combine",
                "print",
                "subsets",
                "splits",
                "target",
                "currentSplitSum",
                "kSubset.sum",
                "sum"
            ]
        }
    },
    {
        "id": "c7eff8c8-1da3-4947-a025-f2c6c6b2a9c8",
        "title": "",
        "chunk_text": "16: schedulingQueues[currentCycle + currentStep].Add(kP erm) ▷Reschedule 17: kP erm.LastDeferralIndex ←kIndex ▷Update continuation index 18: mustExpand ←F alse ▷Drop from current enumeration 19: kIndex ←kIndex + 1 ▷Next element in look-ahead block 20: currentStep ←currentStep + 1 ▷Increment step count 21: return mustExpand 22: 23: // Changes required in the Solver procedure: 24: 25: procedure Solver(S, target) 26: Initialize(S) 27: currentIteration ←0 28: while len(Input) > 0 do ▷Enable multiple iterations 29: if currentIteration > 0 then ▷After the first iteration 30: Input ←futureIterations[currentIteration −1] ▷Use the schedule array 31: for i ←0 to (n/4) −1 do ▷For the required columns in the split 32: Output ←{} 33: for all current ∈Input do 34: [...] 35: expand ←true 36: expand ←ExpandOrSchedule(current, splitElements, currentIteration, MemoizedSums) 37: if expand = true then 38: [...] ▷Proceed with normal expansion 39: currentIteration ←currentIteration + 1 Code Availability The complete source code implementing the algorithms described in this paper will available at https://github.com/jesus-p-salas/subset-sum-solver soon.",
        "metadata": {
            "author": "",
            "keywords": [
                "Input",
                "currentStep",
                "kIndex",
                "Solver",
                "currentIteration",
                "mustExpand",
                "Add",
                "Reschedule",
                "Initialize",
                "Output"
            ]
        }
    }
]